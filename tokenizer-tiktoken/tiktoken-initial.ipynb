{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ef8ae4",
   "metadata": {},
   "source": [
    "## ChatGPT Like\n",
    "\n",
    "using TikToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cdc286d",
   "metadata": {
    "id": "e3e98bac-d43a-40c5-ae98-0afbf5af92d8"
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken -q\n",
    "!pip install matplotlib -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e98bac-d43a-40c5-ae98-0afbf5af92d8",
   "metadata": {
    "id": "e3e98bac-d43a-40c5-ae98-0afbf5af92d8"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa6f915",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3e5387-071a-4ba3-b1ee-d328e21e9106",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf3e5387-071a-4ba3-b1ee-d328e21e9106",
    "outputId": "10262c88-ec0c-4462-f136-80ec27cb8a71"
   },
   "outputs": [],
   "source": [
    "batch_size = 64 #64 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 #256 # what is the maximum context length for predictions?\n",
    "max_iters = 2\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "#torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a9c7642",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a9c7642",
    "outputId": "62dde767-e7af-499a-f381-95dd3e5532fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ef3af",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699f027",
   "metadata": {},
   "source": [
    "Read input + Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8ade7a-81dd-430e-9ec8-430582de178a",
   "metadata": {
    "id": "2b8ade7a-81dd-430e-9ec8-430582de178a"
   },
   "outputs": [],
   "source": [
    "with open('input-fr-tiktoken.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0859ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vocab size:  50257\n"
     ]
    }
   ],
   "source": [
    "tokenizers = tiktoken.get_encoding(\"gpt2\") #gpt2 cl100k_base\n",
    "vocab_size = tokenizers.n_vocab\n",
    "print(\"\\nvocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712356d",
   "metadata": {},
   "source": [
    "Sampling\n",
    "```\n",
    "te = text[20000:20400]\n",
    "len(tokenizers.encode(te))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "iD2NpJRm94Af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iD2NpJRm94Af",
    "outputId": "9561c2c2-5238-4bf2-b0e6-539dfbb9db2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([145781390]) torch.int64\n",
      "tensor([21944,   273, 25930,   628,   198, 35882,  1291,    85,   603,   293,\n",
      "         1834,   390])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(tokenizers.encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:12])\n",
    "\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "#train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e08566",
   "metadata": {},
   "source": [
    "Model + utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86704639-48f8-4800-a9a4-7a45831bc079",
   "metadata": {
    "id": "86704639-48f8-4800-a9a4-7a45831bc079"
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c9c51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_prefix = \"saved-tiktoken-64batch-128block-\"\n",
    "\n",
    "def get_last_backup(backup_prefix):\n",
    "    matching_files = [f for f in os.listdir('.') if os.path.isfile(f) and f.startswith(backup_prefix)]\n",
    "    if (len(matching_files)==0):\n",
    "        return \"\"\n",
    "    return max(matching_files, key=os.path.getmtime)\n",
    "\n",
    "def extract_iteration_count_from(file_name):\n",
    "    match = re.search(r'\\d+(?=-ite)', file_name)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a311bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_losses_to_file(filename, float_list):\n",
    "    # open file in append mode\n",
    "    with open(filename, 'a') as f:\n",
    "        np.savetxt(f, float_list) #.numpy() \n",
    "    # close the file\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ae86d",
   "metadata": {},
   "source": [
    "## Instanciate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96131056",
   "metadata": {},
   "source": [
    "Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0af3b262",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e0e56ad-6416-43ea-bad4-76ad06ea9b80",
    "outputId": "5f491a56-141e-44ef-9633-a8e9018ce260"
   },
   "outputs": [],
   "source": [
    "model = GPTLanguageModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f754cfed",
   "metadata": {},
   "source": [
    "Load weights from previous backup (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35dbd8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved-tiktoken-64batch-128block-3000-ite-x_xx'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_backup_file = get_last_backup(backup_prefix)\n",
    "last_backup_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f70311e9-96c4-4b30-8acf-9cbe200d0383",
   "metadata": {
    "id": "f70311e9-96c4-4b30-8acf-9cbe200d0383",
    "outputId": "63b0d96c-1825-4254-90d0-2796df0c646f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded from saved-tiktoken-64batch-128block-3000-ite-x_xx\n"
     ]
    }
   ],
   "source": [
    "if (len(last_backup_file)!=0):\n",
    "    model.load_state_dict(torch.load(last_backup_file))\n",
    "    model.eval()\n",
    "    print(\"Weights loaded from\", last_backup_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce79421",
   "metadata": {},
   "source": [
    "Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e0e56ad-6416-43ea-bad4-76ad06ea9b80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e0e56ad-6416-43ea-bad4-76ad06ea9b80",
    "outputId": "5f491a56-141e-44ef-9633-a8e9018ce260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.337425 M parameters\n"
     ]
    }
   ],
   "source": [
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc48ee6",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3d1be2f-6bb6-41e9-9937-cc8853dc0cfb",
   "metadata": {
    "id": "a3d1be2f-6bb6-41e9-9937-cc8853dc0cfb"
   },
   "outputs": [],
   "source": [
    "max_iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32c45bd5-712a-49ca-b103-35a63f2bc222",
   "metadata": {
    "id": "32c45bd5-712a-49ca-b103-35a63f2bc222",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.7589, val loss 2.8211\n",
      "Time : 14.703822 seconds\n",
      "step 1: train loss 2.7734, val loss 2.8388\n",
      "Time : 14.191895 seconds\n",
      "step 2: train loss 2.7650, val loss 2.8292\n",
      "Time : 14.192241 seconds\n",
      "step 3: train loss 2.7766, val loss 2.8283\n",
      "Time : 14.158944 seconds\n",
      "step 4: train loss 2.7785, val loss 2.8385\n",
      "Time : 14.080961 seconds\n",
      "step 5: train loss 2.7680, val loss 2.8355\n",
      "Time : 14.043661 seconds\n",
      "step 6: train loss 2.7719, val loss 2.8300\n",
      "Time : 14.101715 seconds\n",
      "step 7: train loss 2.7800, val loss 2.8352\n",
      "Time : 14.072532 seconds\n",
      "step 8: train loss 2.7734, val loss 2.8381\n",
      "Time : 14.024439 seconds\n",
      "step 9: train loss 2.7734, val loss 2.8393\n",
      "Time : 14.021602 seconds\n",
      "step 10: train loss 2.7743, val loss 2.8393\n",
      "Time : 14.255584 seconds\n",
      "step 11: train loss 2.7717, val loss 2.8388\n",
      "Time : 14.302429 seconds\n",
      "step 12: train loss 2.7812, val loss 2.8414\n",
      "Time : 14.113694 seconds\n",
      "step 13: train loss 2.7744, val loss 2.8351\n",
      "Time : 14.051122 seconds\n",
      "step 14: train loss 2.7805, val loss 2.8408\n",
      "Time : 14.085467 seconds\n",
      "step 15: train loss 2.7742, val loss 2.8307\n",
      "Time : 14.130667 seconds\n",
      "step 16: train loss 2.7749, val loss 2.8360\n",
      "Time : 14.078661 seconds\n",
      "step 17: train loss 2.7728, val loss 2.8368\n",
      "Time : 14.055559 seconds\n",
      "step 18: train loss 2.7726, val loss 2.8308\n",
      "Time : 14.140354 seconds\n",
      "step 19: train loss 2.7713, val loss 2.8313\n",
      "Time : 14.251790 seconds\n",
      "step 20: train loss 2.7736, val loss 2.8337\n",
      "Time : 14.235223 seconds\n",
      "step 21: train loss 2.7766, val loss 2.8323\n",
      "Time : 14.078300 seconds\n",
      "step 22: train loss 2.7678, val loss 2.8225\n",
      "Time : 14.106181 seconds\n",
      "step 23: train loss 2.7708, val loss 2.8307\n",
      "Time : 14.108829 seconds\n",
      "step 24: train loss 2.7770, val loss 2.8288\n",
      "Time : 14.126588 seconds\n",
      "step 25: train loss 2.7655, val loss 2.8216\n",
      "Time : 14.142694 seconds\n",
      "step 26: train loss 2.7637, val loss 2.8261\n",
      "Time : 14.120661 seconds\n",
      "step 27: train loss 2.7650, val loss 2.8274\n",
      "Time : 14.273230 seconds\n",
      "step 28: train loss 2.7722, val loss 2.8299\n",
      "Time : 14.243357 seconds\n",
      "step 29: train loss 2.7603, val loss 2.8278\n",
      "Time : 14.124625 seconds\n",
      "step 30: train loss 2.7685, val loss 2.8300\n",
      "Time : 14.108435 seconds\n",
      "step 31: train loss 2.7655, val loss 2.8250\n",
      "Time : 14.135505 seconds\n",
      "step 32: train loss 2.7632, val loss 2.8293\n",
      "Time : 14.152508 seconds\n",
      "step 33: train loss 2.7635, val loss 2.8282\n",
      "Time : 14.125923 seconds\n",
      "step 34: train loss 2.7742, val loss 2.8316\n",
      "Time : 14.102007 seconds\n",
      "step 35: train loss 2.7643, val loss 2.8290\n",
      "Time : 14.159188 seconds\n",
      "step 36: train loss 2.7688, val loss 2.8243\n",
      "Time : 14.321216 seconds\n",
      "step 37: train loss 2.7676, val loss 2.8252\n",
      "Time : 14.247416 seconds\n",
      "step 38: train loss 2.7657, val loss 2.8202\n",
      "Time : 14.083681 seconds\n",
      "step 39: train loss 2.7648, val loss 2.8180\n",
      "Time : 14.092444 seconds\n",
      "step 40: train loss 2.7671, val loss 2.8238\n",
      "Time : 14.166707 seconds\n",
      "step 41: train loss 2.7661, val loss 2.8218\n",
      "Time : 14.156196 seconds\n",
      "step 42: train loss 2.7687, val loss 2.8172\n",
      "Time : 14.090156 seconds\n",
      "step 43: train loss 2.7681, val loss 2.8182\n",
      "Time : 14.088139 seconds\n",
      "step 44: train loss 2.7555, val loss 2.8185\n",
      "Time : 14.252997 seconds\n",
      "step 45: train loss 2.7612, val loss 2.8213\n",
      "Time : 14.280568 seconds\n",
      "step 46: train loss 2.7590, val loss 2.8247\n",
      "Time : 14.137956 seconds\n",
      "step 47: train loss 2.7576, val loss 2.8189\n",
      "Time : 14.094480 seconds\n",
      "step 48: train loss 2.7713, val loss 2.8300\n",
      "Time : 14.101983 seconds\n",
      "step 49: train loss 2.7599, val loss 2.8200\n",
      "Time : 14.106944 seconds\n",
      "step 50: train loss 2.7581, val loss 2.8288\n",
      "Time : 14.100943 seconds\n",
      "step 51: train loss 2.7616, val loss 2.8231\n",
      "Time : 14.113726 seconds\n",
      "step 52: train loss 2.7649, val loss 2.8253\n",
      "Time : 14.185567 seconds\n",
      "step 53: train loss 2.7636, val loss 2.8186\n",
      "Time : 14.264100 seconds\n",
      "step 54: train loss 2.7570, val loss 2.8221\n",
      "Time : 14.249562 seconds\n",
      "step 55: train loss 2.7685, val loss 2.8252\n",
      "Time : 14.096443 seconds\n",
      "step 56: train loss 2.7560, val loss 2.8263\n",
      "Time : 14.109562 seconds\n",
      "step 57: train loss 2.7643, val loss 2.8248\n",
      "Time : 14.119995 seconds\n",
      "step 58: train loss 2.7578, val loss 2.8256\n",
      "Time : 14.131638 seconds\n",
      "step 59: train loss 2.7665, val loss 2.8226\n",
      "Time : 14.100610 seconds\n",
      "step 60: train loss 2.7622, val loss 2.8203\n",
      "Time : 14.085592 seconds\n",
      "step 61: train loss 2.7617, val loss 2.8181\n",
      "Time : 14.337531 seconds\n",
      "step 62: train loss 2.7645, val loss 2.8156\n",
      "Time : 14.362437 seconds\n",
      "step 63: train loss 2.7554, val loss 2.8206\n",
      "Time : 14.105126 seconds\n",
      "step 64: train loss 2.7632, val loss 2.8230\n",
      "Time : 14.081443 seconds\n",
      "step 65: train loss 2.7698, val loss 2.8231\n",
      "Time : 14.133285 seconds\n",
      "step 66: train loss 2.7570, val loss 2.8318\n",
      "Time : 14.132961 seconds\n",
      "step 67: train loss 2.7657, val loss 2.8233\n",
      "Time : 14.092312 seconds\n",
      "step 68: train loss 2.7657, val loss 2.8244\n",
      "Time : 14.080048 seconds\n",
      "step 69: train loss 2.7687, val loss 2.8242\n",
      "Time : 14.182591 seconds\n",
      "step 70: train loss 2.7622, val loss 2.8207\n",
      "Time : 14.304632 seconds\n",
      "step 71: train loss 2.7624, val loss 2.8237\n",
      "Time : 14.217712 seconds\n",
      "step 72: train loss 2.7629, val loss 2.8170\n",
      "Time : 14.099046 seconds\n",
      "step 73: train loss 2.7626, val loss 2.8155\n",
      "Time : 14.099653 seconds\n",
      "step 74: train loss 2.7632, val loss 2.8153\n",
      "Time : 14.123555 seconds\n",
      "step 75: train loss 2.7558, val loss 2.8193\n",
      "Time : 14.100758 seconds\n",
      "step 76: train loss 2.7565, val loss 2.8103\n",
      "Time : 14.117560 seconds\n",
      "step 77: train loss 2.7566, val loss 2.8268\n",
      "Time : 14.091739 seconds\n",
      "step 78: train loss 2.7645, val loss 2.8149\n",
      "Time : 14.304392 seconds\n",
      "step 79: train loss 2.7617, val loss 2.8198\n",
      "Time : 14.270849 seconds\n",
      "step 80: train loss 2.7660, val loss 2.8209\n",
      "Time : 14.129339 seconds\n",
      "step 81: train loss 2.7601, val loss 2.8172\n",
      "Time : 14.103647 seconds\n",
      "step 82: train loss 2.7645, val loss 2.8247\n",
      "Time : 14.137521 seconds\n",
      "step 83: train loss 2.7508, val loss 2.8162\n",
      "Time : 14.127993 seconds\n",
      "step 84: train loss 2.7566, val loss 2.8158\n",
      "Time : 14.108001 seconds\n",
      "step 85: train loss 2.7537, val loss 2.8185\n",
      "Time : 14.094541 seconds\n",
      "step 86: train loss 2.7558, val loss 2.8121\n",
      "Time : 14.241789 seconds\n",
      "step 87: train loss 2.7531, val loss 2.8140\n",
      "Time : 14.331465 seconds\n",
      "step 88: train loss 2.7603, val loss 2.8217\n",
      "Time : 14.235016 seconds\n",
      "step 89: train loss 2.7604, val loss 2.8236\n",
      "Time : 14.079628 seconds\n",
      "step 90: train loss 2.7588, val loss 2.8162\n",
      "Time : 14.113307 seconds\n",
      "step 91: train loss 2.7465, val loss 2.8130\n",
      "Time : 14.170305 seconds\n",
      "step 92: train loss 2.7574, val loss 2.8216\n",
      "Time : 14.120596 seconds\n",
      "step 93: train loss 2.7597, val loss 2.8182\n",
      "Time : 14.084549 seconds\n",
      "step 94: train loss 2.7576, val loss 2.8161\n",
      "Time : 14.099578 seconds\n",
      "step 95: train loss 2.7519, val loss 2.8153\n",
      "Time : 14.289235 seconds\n",
      "step 96: train loss 2.7515, val loss 2.8166\n",
      "Time : 14.305110 seconds\n",
      "step 97: train loss 2.7594, val loss 2.8136\n",
      "Time : 14.096524 seconds\n",
      "step 98: train loss 2.7641, val loss 2.8138\n",
      "Time : 14.098332 seconds\n",
      "step 99: train loss 2.7566, val loss 2.8105\n",
      "Time : 14.136310 seconds\n",
      "step 100: train loss 2.7510, val loss 2.8176\n",
      "Time : 14.134990 seconds\n",
      "step 101: train loss 2.7530, val loss 2.8122\n",
      "Time : 14.140002 seconds\n",
      "step 102: train loss 2.7505, val loss 2.8160\n",
      "Time : 14.116955 seconds\n",
      "step 103: train loss 2.7539, val loss 2.8136\n",
      "Time : 14.211681 seconds\n",
      "step 104: train loss 2.7592, val loss 2.8148\n",
      "Time : 14.209948 seconds\n",
      "step 105: train loss 2.7601, val loss 2.8127\n",
      "Time : 14.173157 seconds\n",
      "step 106: train loss 2.7577, val loss 2.8118\n",
      "Time : 14.117452 seconds\n",
      "step 107: train loss 2.7466, val loss 2.8206\n",
      "Time : 14.118853 seconds\n",
      "step 108: train loss 2.7628, val loss 2.8124\n",
      "Time : 14.160027 seconds\n",
      "step 109: train loss 2.7633, val loss 2.8212\n",
      "Time : 14.128451 seconds\n",
      "step 110: train loss 2.7639, val loss 2.8200\n",
      "Time : 14.111931 seconds\n",
      "step 111: train loss 2.7538, val loss 2.8221\n",
      "Time : 14.135877 seconds\n",
      "step 112: train loss 2.7534, val loss 2.8204\n",
      "Time : 14.343097 seconds\n",
      "step 113: train loss 2.7434, val loss 2.8127\n",
      "Time : 14.351323 seconds\n",
      "step 114: train loss 2.7549, val loss 2.8157\n",
      "Time : 14.103449 seconds\n",
      "step 115: train loss 2.7585, val loss 2.8179\n",
      "Time : 14.104989 seconds\n",
      "step 116: train loss 2.7558, val loss 2.8087\n",
      "Time : 14.175105 seconds\n",
      "step 117: train loss 2.7559, val loss 2.8150\n",
      "Time : 14.167583 seconds\n",
      "step 118: train loss 2.7621, val loss 2.8191\n",
      "Time : 14.113423 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 119: train loss 2.7576, val loss 2.8199\n",
      "Time : 14.100264 seconds\n",
      "step 120: train loss 2.7566, val loss 2.8177\n",
      "Time : 14.249090 seconds\n",
      "step 121: train loss 2.7553, val loss 2.8128\n",
      "Time : 14.310535 seconds\n",
      "step 122: train loss 2.7540, val loss 2.8148\n",
      "Time : 14.222354 seconds\n",
      "step 123: train loss 2.7589, val loss 2.8150\n",
      "Time : 14.125962 seconds\n",
      "step 124: train loss 2.7523, val loss 2.8066\n",
      "Time : 14.148922 seconds\n",
      "step 125: train loss 2.7526, val loss 2.8132\n",
      "Time : 14.138772 seconds\n",
      "step 126: train loss 2.7480, val loss 2.8132\n",
      "Time : 14.152767 seconds\n",
      "step 127: train loss 2.7521, val loss 2.8164\n",
      "Time : 14.158578 seconds\n",
      "step 128: train loss 2.7540, val loss 2.8089\n",
      "Time : 14.184744 seconds\n",
      "step 129: train loss 2.7497, val loss 2.8150\n",
      "Time : 14.327754 seconds\n",
      "step 130: train loss 2.7514, val loss 2.8115\n",
      "Time : 14.309767 seconds\n",
      "step 131: train loss 2.7495, val loss 2.8165\n",
      "Time : 14.123419 seconds\n",
      "step 132: train loss 2.7438, val loss 2.8071\n",
      "Time : 14.132457 seconds\n",
      "step 133: train loss 2.7548, val loss 2.8141\n",
      "Time : 14.161310 seconds\n",
      "step 134: train loss 2.7519, val loss 2.8017\n",
      "Time : 14.161069 seconds\n",
      "step 135: train loss 2.7504, val loss 2.8085\n",
      "Time : 14.112370 seconds\n",
      "step 136: train loss 2.7474, val loss 2.8185\n",
      "Time : 14.104324 seconds\n",
      "step 137: train loss 2.7485, val loss 2.8114\n",
      "Time : 14.291585 seconds\n",
      "step 138: train loss 2.7482, val loss 2.8073\n",
      "Time : 14.337486 seconds\n",
      "step 139: train loss 2.7518, val loss 2.8114\n",
      "Time : 14.181259 seconds\n",
      "step 140: train loss 2.7524, val loss 2.8141\n",
      "Time : 14.092761 seconds\n",
      "step 141: train loss 2.7489, val loss 2.8084\n",
      "Time : 14.133362 seconds\n",
      "step 142: train loss 2.7530, val loss 2.8079\n",
      "Time : 14.159467 seconds\n",
      "step 143: train loss 2.7527, val loss 2.8194\n",
      "Time : 14.137788 seconds\n",
      "step 144: train loss 2.7477, val loss 2.8097\n",
      "Time : 14.100973 seconds\n",
      "step 145: train loss 2.7485, val loss 2.8125\n",
      "Time : 14.151668 seconds\n",
      "step 146: train loss 2.7563, val loss 2.8109\n",
      "Time : 14.306301 seconds\n",
      "step 147: train loss 2.7476, val loss 2.8155\n",
      "Time : 14.258678 seconds\n",
      "step 148: train loss 2.7584, val loss 2.8163\n",
      "Time : 14.144617 seconds\n",
      "step 149: train loss 2.7479, val loss 2.8129\n",
      "Time : 14.158485 seconds\n",
      "step 150: train loss 2.7420, val loss 2.8105\n",
      "Time : 14.146006 seconds\n",
      "step 151: train loss 2.7548, val loss 2.8075\n",
      "Time : 14.138488 seconds\n",
      "step 152: train loss 2.7442, val loss 2.8136\n",
      "Time : 14.130553 seconds\n",
      "step 153: train loss 2.7549, val loss 2.8060\n",
      "Time : 14.137624 seconds\n",
      "step 154: train loss 2.7518, val loss 2.8044\n",
      "Time : 14.280296 seconds\n",
      "step 155: train loss 2.7496, val loss 2.8106\n",
      "Time : 14.306650 seconds\n",
      "step 156: train loss 2.7433, val loss 2.8065\n",
      "Time : 14.177452 seconds\n",
      "step 157: train loss 2.7516, val loss 2.8006\n",
      "Time : 14.117052 seconds\n",
      "step 158: train loss 2.7385, val loss 2.8021\n",
      "Time : 14.133519 seconds\n",
      "step 159: train loss 2.7422, val loss 2.8132\n",
      "Time : 14.143166 seconds\n",
      "step 160: train loss 2.7412, val loss 2.8067\n",
      "Time : 14.133132 seconds\n",
      "step 161: train loss 2.7432, val loss 2.8066\n",
      "Time : 14.106236 seconds\n",
      "step 162: train loss 2.7391, val loss 2.8039\n",
      "Time : 14.186759 seconds\n",
      "step 163: train loss 2.7410, val loss 2.8057\n",
      "Time : 14.358406 seconds\n",
      "step 164: train loss 2.7452, val loss 2.8087\n",
      "Time : 14.302458 seconds\n",
      "step 165: train loss 2.7391, val loss 2.8038\n",
      "Time : 14.100896 seconds\n",
      "step 166: train loss 2.7484, val loss 2.8070\n",
      "Time : 14.109916 seconds\n",
      "step 167: train loss 2.7406, val loss 2.8037\n",
      "Time : 14.167903 seconds\n",
      "step 168: train loss 2.7492, val loss 2.8183\n",
      "Time : 14.148077 seconds\n",
      "step 169: train loss 2.7481, val loss 2.8117\n",
      "Time : 14.100213 seconds\n",
      "step 170: train loss 2.7460, val loss 2.8072\n",
      "Time : 14.104515 seconds\n",
      "step 171: train loss 2.7485, val loss 2.8081\n",
      "Time : 14.266582 seconds\n",
      "step 172: train loss 2.7527, val loss 2.8113\n",
      "Time : 14.337888 seconds\n",
      "step 173: train loss 2.7533, val loss 2.8115\n",
      "Time : 14.168545 seconds\n",
      "step 174: train loss 2.7455, val loss 2.8056\n",
      "Time : 14.115400 seconds\n",
      "step 175: train loss 2.7454, val loss 2.8028\n",
      "Time : 14.125872 seconds\n",
      "step 176: train loss 2.7506, val loss 2.8137\n",
      "Time : 14.143952 seconds\n",
      "step 177: train loss 2.7496, val loss 2.8121\n",
      "Time : 14.176118 seconds\n",
      "step 178: train loss 2.7431, val loss 2.8096\n",
      "Time : 14.147939 seconds\n",
      "step 179: train loss 2.7499, val loss 2.8088\n",
      "Time : 14.231910 seconds\n",
      "step 180: train loss 2.7472, val loss 2.8084\n",
      "Time : 14.314088 seconds\n",
      "step 181: train loss 2.7430, val loss 2.8062\n",
      "Time : 14.280420 seconds\n",
      "step 182: train loss 2.7399, val loss 2.8048\n",
      "Time : 14.136340 seconds\n",
      "step 183: train loss 2.7469, val loss 2.8069\n",
      "Time : 14.145314 seconds\n",
      "step 184: train loss 2.7451, val loss 2.8074\n",
      "Time : 14.181391 seconds\n",
      "step 185: train loss 2.7436, val loss 2.8038\n",
      "Time : 14.159493 seconds\n",
      "step 186: train loss 2.7421, val loss 2.8121\n",
      "Time : 14.112605 seconds\n",
      "step 187: train loss 2.7433, val loss 2.7995\n",
      "Time : 14.093800 seconds\n",
      "step 188: train loss 2.7391, val loss 2.8052\n",
      "Time : 14.366035 seconds\n",
      "step 189: train loss 2.7421, val loss 2.8104\n",
      "Time : 14.357855 seconds\n",
      "step 190: train loss 2.7471, val loss 2.8064\n",
      "Time : 14.127019 seconds\n",
      "step 191: train loss 2.7503, val loss 2.8164\n",
      "Time : 14.109378 seconds\n",
      "step 192: train loss 2.7540, val loss 2.8170\n",
      "Time : 14.172990 seconds\n",
      "step 193: train loss 2.7513, val loss 2.8192\n",
      "Time : 14.167971 seconds\n",
      "step 194: train loss 2.7506, val loss 2.8105\n",
      "Time : 14.100433 seconds\n",
      "step 195: train loss 2.7404, val loss 2.8186\n",
      "Time : 14.101523 seconds\n",
      "step 196: train loss 2.7473, val loss 2.8074\n",
      "Time : 14.204508 seconds\n",
      "step 197: train loss 2.7406, val loss 2.7966\n",
      "Time : 14.288364 seconds\n",
      "step 198: train loss 2.7356, val loss 2.8052\n",
      "Time : 14.253111 seconds\n",
      "step 199: train loss 2.7420, val loss 2.7999\n",
      "Time : 14.112837 seconds\n",
      "step 200: train loss 2.7470, val loss 2.8117\n",
      "Time : 14.112016 seconds\n",
      "step 201: train loss 2.7371, val loss 2.8049\n",
      "Time : 14.161331 seconds\n",
      "step 202: train loss 2.7449, val loss 2.7991\n",
      "Time : 14.164639 seconds\n",
      "step 203: train loss 2.7381, val loss 2.8049\n",
      "Time : 14.127381 seconds\n",
      "step 204: train loss 2.7403, val loss 2.8090\n",
      "Time : 14.168402 seconds\n",
      "step 205: train loss 2.7421, val loss 2.8081\n",
      "Time : 14.287806 seconds\n",
      "step 206: train loss 2.7404, val loss 2.8069\n",
      "Time : 14.313789 seconds\n",
      "step 207: train loss 2.7426, val loss 2.8083\n",
      "Time : 14.124747 seconds\n",
      "step 208: train loss 2.7446, val loss 2.8050\n",
      "Time : 14.122108 seconds\n",
      "step 209: train loss 2.7470, val loss 2.8074\n",
      "Time : 14.162474 seconds\n",
      "step 210: train loss 2.7454, val loss 2.8019\n",
      "Time : 14.169173 seconds\n",
      "step 211: train loss 2.7396, val loss 2.8058\n",
      "Time : 14.158420 seconds\n",
      "step 212: train loss 2.7328, val loss 2.8010\n",
      "Time : 14.113761 seconds\n",
      "step 213: train loss 2.7312, val loss 2.7935\n",
      "Time : 14.245746 seconds\n",
      "step 214: train loss 2.7394, val loss 2.8080\n",
      "Time : 14.378241 seconds\n",
      "step 215: train loss 2.7382, val loss 2.8095\n",
      "Time : 14.238873 seconds\n",
      "step 216: train loss 2.7374, val loss 2.8078\n",
      "Time : 14.108295 seconds\n",
      "step 217: train loss 2.7470, val loss 2.8020\n",
      "Time : 14.120456 seconds\n",
      "step 218: train loss 2.7367, val loss 2.8021\n",
      "Time : 14.164769 seconds\n",
      "step 219: train loss 2.7382, val loss 2.7975\n",
      "Time : 14.162515 seconds\n",
      "step 220: train loss 2.7443, val loss 2.8021\n",
      "Time : 14.108274 seconds\n",
      "step 221: train loss 2.7448, val loss 2.8027\n",
      "Time : 14.127733 seconds\n",
      "step 222: train loss 2.7411, val loss 2.8070\n",
      "Time : 14.309663 seconds\n",
      "step 223: train loss 2.7498, val loss 2.8046\n",
      "Time : 14.306195 seconds\n",
      "step 224: train loss 2.7481, val loss 2.8072\n",
      "Time : 14.097112 seconds\n",
      "step 225: train loss 2.7377, val loss 2.8084\n",
      "Time : 14.121432 seconds\n",
      "step 226: train loss 2.7377, val loss 2.8030\n",
      "Time : 14.129370 seconds\n",
      "step 227: train loss 2.7320, val loss 2.7902\n",
      "Time : 14.132116 seconds\n",
      "step 228: train loss 2.7407, val loss 2.7980\n",
      "Time : 14.139324 seconds\n",
      "step 229: train loss 2.7391, val loss 2.8021\n",
      "Time : 14.137101 seconds\n",
      "step 230: train loss 2.7427, val loss 2.8004\n",
      "Time : 14.263870 seconds\n",
      "step 231: train loss 2.7438, val loss 2.8025\n",
      "Time : 14.327343 seconds\n",
      "step 232: train loss 2.7385, val loss 2.7976\n",
      "Time : 14.202752 seconds\n",
      "step 233: train loss 2.7335, val loss 2.7986\n",
      "Time : 14.116905 seconds\n",
      "step 234: train loss 2.7406, val loss 2.8029\n",
      "Time : 14.137595 seconds\n",
      "step 235: train loss 2.7350, val loss 2.7963\n",
      "Time : 14.169534 seconds\n",
      "step 236: train loss 2.7314, val loss 2.8035\n",
      "Time : 14.157314 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 237: train loss 2.7405, val loss 2.8043\n",
      "Time : 14.096463 seconds\n",
      "step 238: train loss 2.7430, val loss 2.8086\n",
      "Time : 14.111556 seconds\n",
      "step 239: train loss 2.7364, val loss 2.8051\n",
      "Time : 14.362569 seconds\n",
      "step 240: train loss 2.7400, val loss 2.8003\n",
      "Time : 14.325330 seconds\n",
      "step 241: train loss 2.7326, val loss 2.7981\n",
      "Time : 14.135865 seconds\n",
      "step 242: train loss 2.7323, val loss 2.7952\n",
      "Time : 14.101346 seconds\n",
      "step 243: train loss 2.7351, val loss 2.7969\n",
      "Time : 14.151572 seconds\n",
      "step 244: train loss 2.7318, val loss 2.7962\n",
      "Time : 14.147619 seconds\n",
      "step 245: train loss 2.7295, val loss 2.8024\n",
      "Time : 14.103273 seconds\n",
      "step 246: train loss 2.7376, val loss 2.8017\n",
      "Time : 14.086688 seconds\n",
      "step 247: train loss 2.7411, val loss 2.8065\n",
      "Time : 14.246077 seconds\n",
      "step 248: train loss 2.7375, val loss 2.8095\n",
      "Time : 14.310622 seconds\n",
      "step 249: train loss 2.7353, val loss 2.8010\n",
      "Time : 14.196847 seconds\n",
      "step 250: train loss 2.7400, val loss 2.8090\n",
      "Time : 14.116480 seconds\n",
      "step 251: train loss 2.7360, val loss 2.7965\n",
      "Time : 14.120259 seconds\n",
      "step 252: train loss 2.7344, val loss 2.8069\n",
      "Time : 14.148784 seconds\n",
      "step 253: train loss 2.7399, val loss 2.8046\n",
      "Time : 14.136973 seconds\n",
      "step 254: train loss 2.7452, val loss 2.7947\n",
      "Time : 14.125444 seconds\n",
      "step 255: train loss 2.7392, val loss 2.8087\n",
      "Time : 14.180276 seconds\n",
      "step 256: train loss 2.7419, val loss 2.7932\n",
      "Time : 14.285780 seconds\n",
      "step 257: train loss 2.7396, val loss 2.8019\n",
      "Time : 14.273913 seconds\n",
      "step 258: train loss 2.7398, val loss 2.8060\n",
      "Time : 14.120438 seconds\n",
      "step 259: train loss 2.7421, val loss 2.8035\n",
      "Time : 14.109092 seconds\n",
      "step 260: train loss 2.7387, val loss 2.7921\n",
      "Time : 14.154951 seconds\n",
      "step 261: train loss 2.7345, val loss 2.7966\n",
      "Time : 14.144536 seconds\n",
      "step 262: train loss 2.7460, val loss 2.8048\n",
      "Time : 14.104823 seconds\n",
      "step 263: train loss 2.7352, val loss 2.7956\n",
      "Time : 14.110884 seconds\n",
      "step 264: train loss 2.7284, val loss 2.7955\n",
      "Time : 14.300035 seconds\n",
      "step 265: train loss 2.7313, val loss 2.7942\n",
      "Time : 14.346617 seconds\n",
      "step 266: train loss 2.7330, val loss 2.7985\n",
      "Time : 14.182531 seconds\n",
      "step 267: train loss 2.7397, val loss 2.7996\n",
      "Time : 14.091325 seconds\n",
      "step 268: train loss 2.7411, val loss 2.7914\n",
      "Time : 14.131220 seconds\n",
      "step 269: train loss 2.7371, val loss 2.8048\n",
      "Time : 14.164261 seconds\n",
      "step 270: train loss 2.7359, val loss 2.7923\n",
      "Time : 14.140034 seconds\n",
      "step 271: train loss 2.7387, val loss 2.7984\n",
      "Time : 14.079752 seconds\n",
      "step 272: train loss 2.7355, val loss 2.7919\n",
      "Time : 14.149718 seconds\n",
      "step 273: train loss 2.7369, val loss 2.8028\n",
      "Time : 14.300123 seconds\n",
      "step 274: train loss 2.7341, val loss 2.8014\n",
      "Time : 14.255900 seconds\n",
      "step 275: train loss 2.7331, val loss 2.7964\n",
      "Time : 14.097445 seconds\n",
      "step 276: train loss 2.7272, val loss 2.7986\n",
      "Time : 14.113205 seconds\n",
      "step 277: train loss 2.7345, val loss 2.7918\n",
      "Time : 14.138150 seconds\n",
      "step 278: train loss 2.7383, val loss 2.7943\n",
      "Time : 14.136997 seconds\n",
      "step 279: train loss 2.7375, val loss 2.7922\n",
      "Time : 14.132940 seconds\n",
      "step 280: train loss 2.7417, val loss 2.7925\n",
      "Time : 14.142381 seconds\n",
      "step 281: train loss 2.7331, val loss 2.8039\n",
      "Time : 14.280483 seconds\n",
      "step 282: train loss 2.7284, val loss 2.7974\n",
      "Time : 14.319870 seconds\n",
      "step 283: train loss 2.7323, val loss 2.7957\n",
      "Time : 14.163020 seconds\n",
      "step 284: train loss 2.7305, val loss 2.7978\n",
      "Time : 14.144006 seconds\n",
      "step 285: train loss 2.7371, val loss 2.8020\n",
      "Time : 14.141401 seconds\n",
      "step 286: train loss 2.7385, val loss 2.8051\n",
      "Time : 14.149350 seconds\n",
      "step 287: train loss 2.7343, val loss 2.7971\n",
      "Time : 14.107759 seconds\n",
      "step 288: train loss 2.7357, val loss 2.7973\n",
      "Time : 14.129197 seconds\n",
      "step 289: train loss 2.7312, val loss 2.7925\n",
      "Time : 14.208390 seconds\n",
      "step 290: train loss 2.7351, val loss 2.7975\n",
      "Time : 14.339701 seconds\n",
      "step 291: train loss 2.7289, val loss 2.7960\n",
      "Time : 14.288450 seconds\n",
      "step 292: train loss 2.7280, val loss 2.7980\n",
      "Time : 14.076061 seconds\n",
      "step 293: train loss 2.7297, val loss 2.7921\n",
      "Time : 14.094584 seconds\n",
      "step 294: train loss 2.7302, val loss 2.7976\n",
      "Time : 14.128769 seconds\n",
      "step 295: train loss 2.7317, val loss 2.7930\n",
      "Time : 14.136697 seconds\n",
      "step 296: train loss 2.7338, val loss 2.7979\n",
      "Time : 14.099319 seconds\n",
      "step 297: train loss 2.7413, val loss 2.7913\n",
      "Time : 14.077710 seconds\n",
      "step 298: train loss 2.7296, val loss 2.8017\n",
      "Time : 14.294332 seconds\n",
      "step 299: train loss 2.7263, val loss 2.7984\n",
      "Time : 14.292555 seconds\n",
      "step 300: train loss 2.7344, val loss 2.7941\n",
      "Time : 14.144395 seconds\n",
      "step 301: train loss 2.7283, val loss 2.7895\n",
      "Time : 14.101665 seconds\n",
      "step 302: train loss 2.7335, val loss 2.7928\n",
      "Time : 14.119481 seconds\n",
      "step 303: train loss 2.7321, val loss 2.7950\n",
      "Time : 14.135346 seconds\n",
      "step 304: train loss 2.7260, val loss 2.7982\n",
      "Time : 14.125639 seconds\n",
      "step 305: train loss 2.7307, val loss 2.7981\n",
      "Time : 14.145254 seconds\n",
      "step 306: train loss 2.7272, val loss 2.8001\n",
      "Time : 14.216040 seconds\n",
      "step 307: train loss 2.7326, val loss 2.8000\n",
      "Time : 14.319211 seconds\n",
      "step 308: train loss 2.7335, val loss 2.8018\n",
      "Time : 14.252087 seconds\n",
      "step 309: train loss 2.7286, val loss 2.7968\n",
      "Time : 14.116292 seconds\n",
      "step 310: train loss 2.7259, val loss 2.7986\n",
      "Time : 14.134342 seconds\n",
      "step 311: train loss 2.7331, val loss 2.7917\n",
      "Time : 14.151472 seconds\n",
      "step 312: train loss 2.7298, val loss 2.7886\n",
      "Time : 14.158858 seconds\n",
      "step 313: train loss 2.7403, val loss 2.7998\n",
      "Time : 14.099651 seconds\n",
      "step 314: train loss 2.7278, val loss 2.7958\n",
      "Time : 14.110533 seconds\n",
      "step 315: train loss 2.7341, val loss 2.7973\n",
      "Time : 14.333128 seconds\n",
      "step 316: train loss 2.7298, val loss 2.8028\n",
      "Time : 14.334940 seconds\n",
      "step 317: train loss 2.7341, val loss 2.8123\n",
      "Time : 14.126843 seconds\n",
      "step 318: train loss 2.7420, val loss 2.7983\n",
      "Time : 14.105125 seconds\n",
      "step 319: train loss 2.7331, val loss 2.7953\n",
      "Time : 14.170571 seconds\n",
      "step 320: train loss 2.7320, val loss 2.7947\n",
      "Time : 14.179263 seconds\n",
      "step 321: train loss 2.7332, val loss 2.7970\n",
      "Time : 14.140131 seconds\n",
      "step 322: train loss 2.7295, val loss 2.8046\n",
      "Time : 14.102955 seconds\n",
      "step 323: train loss 2.7267, val loss 2.7935\n",
      "Time : 14.210747 seconds\n",
      "step 324: train loss 2.7224, val loss 2.7932\n",
      "Time : 14.303583 seconds\n",
      "step 325: train loss 2.7320, val loss 2.7909\n",
      "Time : 14.227673 seconds\n",
      "step 326: train loss 2.7381, val loss 2.7984\n",
      "Time : 14.115256 seconds\n",
      "step 327: train loss 2.7273, val loss 2.7923\n",
      "Time : 14.109862 seconds\n",
      "step 328: train loss 2.7349, val loss 2.7970\n",
      "Time : 14.158026 seconds\n",
      "step 329: train loss 2.7278, val loss 2.7932\n",
      "Time : 14.127288 seconds\n",
      "step 330: train loss 2.7320, val loss 2.7991\n",
      "Time : 14.146172 seconds\n",
      "step 331: train loss 2.7413, val loss 2.7947\n",
      "Time : 14.145971 seconds\n",
      "step 332: train loss 2.7353, val loss 2.7966\n",
      "Time : 14.274169 seconds\n",
      "step 333: train loss 2.7279, val loss 2.8015\n",
      "Time : 14.266671 seconds\n",
      "step 334: train loss 2.7288, val loss 2.7969\n",
      "Time : 14.134091 seconds\n",
      "step 335: train loss 2.7270, val loss 2.7926\n",
      "Time : 14.129615 seconds\n",
      "step 336: train loss 2.7300, val loss 2.7908\n",
      "Time : 14.193447 seconds\n",
      "step 337: train loss 2.7319, val loss 2.7885\n",
      "Time : 14.159033 seconds\n",
      "step 338: train loss 2.7235, val loss 2.7956\n",
      "Time : 14.136369 seconds\n",
      "step 339: train loss 2.7310, val loss 2.7970\n",
      "Time : 14.122782 seconds\n",
      "step 340: train loss 2.7242, val loss 2.7854\n",
      "Time : 14.272469 seconds\n",
      "step 341: train loss 2.7277, val loss 2.7937\n",
      "Time : 14.356274 seconds\n",
      "step 342: train loss 2.7276, val loss 2.7897\n",
      "Time : 14.229882 seconds\n",
      "step 343: train loss 2.7307, val loss 2.7965\n",
      "Time : 14.099671 seconds\n",
      "step 344: train loss 2.7275, val loss 2.7872\n",
      "Time : 14.109778 seconds\n",
      "step 345: train loss 2.7235, val loss 2.7976\n",
      "Time : 14.185948 seconds\n",
      "step 346: train loss 2.7337, val loss 2.7930\n",
      "Time : 14.183462 seconds\n",
      "step 347: train loss 2.7244, val loss 2.7933\n",
      "Time : 14.104238 seconds\n",
      "step 348: train loss 2.7281, val loss 2.7960\n",
      "Time : 14.123889 seconds\n",
      "step 349: train loss 2.7235, val loss 2.7976\n",
      "Time : 14.289519 seconds\n",
      "step 350: train loss 2.7232, val loss 2.7969\n",
      "Time : 14.308278 seconds\n",
      "step 351: train loss 2.7247, val loss 2.7912\n",
      "Time : 14.111844 seconds\n",
      "step 352: train loss 2.7248, val loss 2.7881\n",
      "Time : 14.123004 seconds\n",
      "step 353: train loss 2.7290, val loss 2.7959\n",
      "Time : 14.157310 seconds\n",
      "step 354: train loss 2.7367, val loss 2.8002\n",
      "Time : 14.141182 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 355: train loss 2.7346, val loss 2.7864\n",
      "Time : 14.146281 seconds\n",
      "step 356: train loss 2.7207, val loss 2.7942\n",
      "Time : 14.160025 seconds\n",
      "step 357: train loss 2.7204, val loss 2.7884\n",
      "Time : 14.235042 seconds\n",
      "step 358: train loss 2.7266, val loss 2.7918\n",
      "Time : 14.250906 seconds\n",
      "step 359: train loss 2.7204, val loss 2.7867\n",
      "Time : 14.168601 seconds\n",
      "step 360: train loss 2.7301, val loss 2.7945\n",
      "Time : 14.133622 seconds\n",
      "step 361: train loss 2.7357, val loss 2.7991\n",
      "Time : 14.166109 seconds\n",
      "step 362: train loss 2.7215, val loss 2.7929\n",
      "Time : 14.170804 seconds\n",
      "step 363: train loss 2.7267, val loss 2.8007\n",
      "Time : 14.144182 seconds\n",
      "step 364: train loss 2.7249, val loss 2.7940\n",
      "Time : 14.098826 seconds\n",
      "step 365: train loss 2.7262, val loss 2.7874\n",
      "Time : 14.142119 seconds\n",
      "step 366: train loss 2.7300, val loss 2.7890\n",
      "Time : 14.372785 seconds\n",
      "step 367: train loss 2.7184, val loss 2.7803\n",
      "Time : 14.337097 seconds\n",
      "step 368: train loss 2.7187, val loss 2.7862\n",
      "Time : 14.108524 seconds\n",
      "step 369: train loss 2.7265, val loss 2.7918\n",
      "Time : 14.109332 seconds\n",
      "step 370: train loss 2.7286, val loss 2.7904\n",
      "Time : 14.184741 seconds\n",
      "step 371: train loss 2.7246, val loss 2.7935\n",
      "Time : 14.195792 seconds\n",
      "step 372: train loss 2.7239, val loss 2.7909\n",
      "Time : 14.115731 seconds\n",
      "step 373: train loss 2.7257, val loss 2.7928\n",
      "Time : 14.099819 seconds\n",
      "step 374: train loss 2.7211, val loss 2.7911\n",
      "Time : 14.235500 seconds\n",
      "step 375: train loss 2.7217, val loss 2.7933\n",
      "Time : 14.310193 seconds\n",
      "step 376: train loss 2.7222, val loss 2.7912\n",
      "Time : 14.174407 seconds\n",
      "step 377: train loss 2.7288, val loss 2.7931\n",
      "Time : 14.110228 seconds\n",
      "step 378: train loss 2.7248, val loss 2.7986\n",
      "Time : 14.107675 seconds\n",
      "step 379: train loss 2.7262, val loss 2.7912\n",
      "Time : 14.155522 seconds\n",
      "step 380: train loss 2.7292, val loss 2.7977\n",
      "Time : 14.134271 seconds\n",
      "step 381: train loss 2.7256, val loss 2.7860\n",
      "Time : 14.147581 seconds\n",
      "step 382: train loss 2.7191, val loss 2.7881\n",
      "Time : 14.173688 seconds\n",
      "step 383: train loss 2.7232, val loss 2.7855\n",
      "Time : 14.303013 seconds\n",
      "step 384: train loss 2.7179, val loss 2.7830\n",
      "Time : 14.247187 seconds\n",
      "step 385: train loss 2.7272, val loss 2.7864\n",
      "Time : 14.116237 seconds\n",
      "step 386: train loss 2.7262, val loss 2.7833\n",
      "Time : 14.155916 seconds\n",
      "step 387: train loss 2.7252, val loss 2.7893\n",
      "Time : 14.157740 seconds\n",
      "step 388: train loss 2.7187, val loss 2.7919\n",
      "Time : 14.161185 seconds\n",
      "step 389: train loss 2.7239, val loss 2.7897\n",
      "Time : 14.104539 seconds\n",
      "step 390: train loss 2.7225, val loss 2.7908\n",
      "Time : 14.137801 seconds\n",
      "step 391: train loss 2.7246, val loss 2.7900\n",
      "Time : 14.349556 seconds\n",
      "step 392: train loss 2.7219, val loss 2.7881\n",
      "Time : 14.359934 seconds\n",
      "step 393: train loss 2.7223, val loss 2.7878\n",
      "Time : 14.172230 seconds\n",
      "step 394: train loss 2.7222, val loss 2.7828\n",
      "Time : 14.101753 seconds\n",
      "step 395: train loss 2.7174, val loss 2.7915\n",
      "Time : 14.129717 seconds\n",
      "step 396: train loss 2.7297, val loss 2.7753\n",
      "Time : 14.178127 seconds\n",
      "step 397: train loss 2.7242, val loss 2.7854\n",
      "Time : 14.148869 seconds\n",
      "step 398: train loss 2.7238, val loss 2.7896\n",
      "Time : 14.100733 seconds\n",
      "step 399: train loss 2.7238, val loss 2.7886\n",
      "Time : 14.172584 seconds\n",
      "step 400: train loss 2.7211, val loss 2.7873\n",
      "Time : 14.284947 seconds\n",
      "step 401: train loss 2.7306, val loss 2.7922\n",
      "Time : 14.255602 seconds\n",
      "step 402: train loss 2.7197, val loss 2.7888\n",
      "Time : 14.122589 seconds\n",
      "step 403: train loss 2.7267, val loss 2.7830\n",
      "Time : 14.101730 seconds\n",
      "step 404: train loss 2.7201, val loss 2.7885\n",
      "Time : 14.119129 seconds\n",
      "step 405: train loss 2.7265, val loss 2.7876\n",
      "Time : 14.136837 seconds\n",
      "step 406: train loss 2.7231, val loss 2.7847\n",
      "Time : 14.169091 seconds\n",
      "step 407: train loss 2.7199, val loss 2.7898\n",
      "Time : 14.136812 seconds\n",
      "step 408: train loss 2.7229, val loss 2.7867\n",
      "Time : 14.299726 seconds\n",
      "step 409: train loss 2.7173, val loss 2.7835\n",
      "Time : 14.272663 seconds\n",
      "step 410: train loss 2.7207, val loss 2.7811\n",
      "Time : 14.169651 seconds\n",
      "step 411: train loss 2.7223, val loss 2.7839\n",
      "Time : 14.119460 seconds\n",
      "step 412: train loss 2.7224, val loss 2.7879\n",
      "Time : 14.153061 seconds\n",
      "step 413: train loss 2.7242, val loss 2.7860\n",
      "Time : 14.146271 seconds\n",
      "step 414: train loss 2.7210, val loss 2.7833\n",
      "Time : 14.132235 seconds\n",
      "step 415: train loss 2.7161, val loss 2.7868\n",
      "Time : 14.111525 seconds\n",
      "step 416: train loss 2.7316, val loss 2.7927\n",
      "Time : 14.189938 seconds\n",
      "step 417: train loss 2.7261, val loss 2.7813\n",
      "Time : 14.339092 seconds\n",
      "step 418: train loss 2.7208, val loss 2.7838\n",
      "Time : 14.276388 seconds\n",
      "step 419: train loss 2.7166, val loss 2.7807\n",
      "Time : 14.093698 seconds\n",
      "step 420: train loss 2.7182, val loss 2.7799\n",
      "Time : 14.118026 seconds\n",
      "step 421: train loss 2.7168, val loss 2.7853\n",
      "Time : 14.164080 seconds\n",
      "step 422: train loss 2.7193, val loss 2.7878\n",
      "Time : 14.167372 seconds\n",
      "step 423: train loss 2.7187, val loss 2.7922\n",
      "Time : 14.095164 seconds\n",
      "step 424: train loss 2.7233, val loss 2.7906\n",
      "Time : 14.093696 seconds\n",
      "step 425: train loss 2.7147, val loss 2.7779\n",
      "Time : 14.304021 seconds\n",
      "step 426: train loss 2.7126, val loss 2.7850\n",
      "Time : 14.344491 seconds\n",
      "step 427: train loss 2.7194, val loss 2.7881\n",
      "Time : 14.175346 seconds\n",
      "step 428: train loss 2.7205, val loss 2.7774\n",
      "Time : 14.105488 seconds\n",
      "step 429: train loss 2.7179, val loss 2.7853\n",
      "Time : 14.134952 seconds\n",
      "step 430: train loss 2.7225, val loss 2.7814\n",
      "Time : 14.142838 seconds\n",
      "step 431: train loss 2.7209, val loss 2.7766\n",
      "Time : 14.135998 seconds\n",
      "step 432: train loss 2.7138, val loss 2.7812\n",
      "Time : 14.141511 seconds\n",
      "step 433: train loss 2.7175, val loss 2.7741\n",
      "Time : 14.226810 seconds\n",
      "step 434: train loss 2.7129, val loss 2.7838\n",
      "Time : 14.297930 seconds\n",
      "step 435: train loss 2.7189, val loss 2.7816\n",
      "Time : 14.236830 seconds\n",
      "step 436: train loss 2.7239, val loss 2.7752\n",
      "Time : 14.131225 seconds\n",
      "step 437: train loss 2.7255, val loss 2.7837\n",
      "Time : 14.144576 seconds\n",
      "step 438: train loss 2.7212, val loss 2.7824\n",
      "Time : 14.169226 seconds\n",
      "step 439: train loss 2.7163, val loss 2.7790\n",
      "Time : 14.160738 seconds\n",
      "step 440: train loss 2.7127, val loss 2.7848\n",
      "Time : 14.103965 seconds\n",
      "step 441: train loss 2.7214, val loss 2.7856\n",
      "Time : 14.101038 seconds\n",
      "step 442: train loss 2.7187, val loss 2.7897\n",
      "Time : 14.359135 seconds\n",
      "step 443: train loss 2.7195, val loss 2.7849\n",
      "Time : 14.327214 seconds\n",
      "step 444: train loss 2.7179, val loss 2.7831\n",
      "Time : 14.121142 seconds\n",
      "step 445: train loss 2.7149, val loss 2.7849\n",
      "Time : 14.135612 seconds\n",
      "step 446: train loss 2.7157, val loss 2.7851\n",
      "Time : 14.153873 seconds\n",
      "step 447: train loss 2.7192, val loss 2.7838\n",
      "Time : 14.178632 seconds\n",
      "step 448: train loss 2.7158, val loss 2.7762\n",
      "Time : 14.127894 seconds\n",
      "step 449: train loss 2.7112, val loss 2.7817\n",
      "Time : 14.095561 seconds\n",
      "step 450: train loss 2.7136, val loss 2.7753\n",
      "Time : 14.225919 seconds\n",
      "step 451: train loss 2.7213, val loss 2.7766\n",
      "Time : 14.296261 seconds\n",
      "step 452: train loss 2.7122, val loss 2.7835\n",
      "Time : 14.206538 seconds\n",
      "step 453: train loss 2.7177, val loss 2.7779\n",
      "Time : 14.094735 seconds\n",
      "step 454: train loss 2.7171, val loss 2.7845\n",
      "Time : 14.114683 seconds\n",
      "step 455: train loss 2.7199, val loss 2.7873\n",
      "Time : 14.143715 seconds\n",
      "step 456: train loss 2.7218, val loss 2.7828\n",
      "Time : 14.142200 seconds\n",
      "step 457: train loss 2.7161, val loss 2.7805\n",
      "Time : 14.159256 seconds\n",
      "step 458: train loss 2.7202, val loss 2.7764\n",
      "Time : 14.153877 seconds\n",
      "step 459: train loss 2.7205, val loss 2.7775\n",
      "Time : 14.288332 seconds\n",
      "step 460: train loss 2.7154, val loss 2.7844\n",
      "Time : 14.311226 seconds\n",
      "step 461: train loss 2.7173, val loss 2.7830\n",
      "Time : 14.120935 seconds\n",
      "step 462: train loss 2.7204, val loss 2.7762\n",
      "Time : 14.113017 seconds\n",
      "step 463: train loss 2.7184, val loss 2.7828\n",
      "Time : 14.182100 seconds\n",
      "step 464: train loss 2.7106, val loss 2.7810\n",
      "Time : 14.175662 seconds\n",
      "step 465: train loss 2.7165, val loss 2.7846\n",
      "Time : 14.131556 seconds\n",
      "step 466: train loss 2.7190, val loss 2.7745\n",
      "Time : 14.108695 seconds\n",
      "step 467: train loss 2.7197, val loss 2.7853\n",
      "Time : 14.267317 seconds\n",
      "step 468: train loss 2.7156, val loss 2.7824\n",
      "Time : 14.359251 seconds\n",
      "step 469: train loss 2.7127, val loss 2.7803\n",
      "Time : 14.211804 seconds\n",
      "step 470: train loss 2.7192, val loss 2.7787\n",
      "Time : 14.099568 seconds\n",
      "step 471: train loss 2.7168, val loss 2.7776\n",
      "Time : 14.120876 seconds\n",
      "step 472: train loss 2.7158, val loss 2.7856\n",
      "Time : 14.175521 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 473: train loss 2.7121, val loss 2.7851\n",
      "Time : 14.155933 seconds\n",
      "step 474: train loss 2.7168, val loss 2.7851\n",
      "Time : 14.111325 seconds\n",
      "step 475: train loss 2.7094, val loss 2.7857\n",
      "Time : 14.193149 seconds\n",
      "step 476: train loss 2.7236, val loss 2.7814\n",
      "Time : 15.013141 seconds\n",
      "step 477: train loss 2.7123, val loss 2.7846\n",
      "Time : 14.439147 seconds\n",
      "step 478: train loss 2.7162, val loss 2.7787\n",
      "Time : 14.240706 seconds\n",
      "step 479: train loss 2.7140, val loss 2.7836\n",
      "Time : 14.131347 seconds\n",
      "step 480: train loss 2.7091, val loss 2.7845\n",
      "Time : 14.201088 seconds\n",
      "step 481: train loss 2.7102, val loss 2.7785\n",
      "Time : 14.172351 seconds\n",
      "step 482: train loss 2.7169, val loss 2.7833\n",
      "Time : 14.165928 seconds\n",
      "step 483: train loss 2.7189, val loss 2.7844\n",
      "Time : 14.169143 seconds\n",
      "step 484: train loss 2.7187, val loss 2.7843\n",
      "Time : 14.301481 seconds\n",
      "step 485: train loss 2.7149, val loss 2.7818\n",
      "Time : 14.313829 seconds\n",
      "step 486: train loss 2.7178, val loss 2.7889\n",
      "Time : 14.205790 seconds\n",
      "step 487: train loss 2.7152, val loss 2.7783\n",
      "Time : 14.140660 seconds\n",
      "step 488: train loss 2.7178, val loss 2.7771\n",
      "Time : 14.182090 seconds\n",
      "step 489: train loss 2.7054, val loss 2.7834\n",
      "Time : 14.174716 seconds\n",
      "step 490: train loss 2.7118, val loss 2.7778\n",
      "Time : 14.207434 seconds\n",
      "step 491: train loss 2.7121, val loss 2.7771\n",
      "Time : 14.175861 seconds\n",
      "step 492: train loss 2.7176, val loss 2.7774\n",
      "Time : 14.205157 seconds\n",
      "step 493: train loss 2.7172, val loss 2.7852\n",
      "Time : 14.381147 seconds\n",
      "step 494: train loss 2.7124, val loss 2.7802\n",
      "Time : 14.301720 seconds\n",
      "step 495: train loss 2.7158, val loss 2.7799\n",
      "Time : 14.137970 seconds\n",
      "step 496: train loss 2.7118, val loss 2.7761\n",
      "Time : 14.145823 seconds\n",
      "step 497: train loss 2.7167, val loss 2.7750\n",
      "Time : 14.234154 seconds\n",
      "step 498: train loss 2.7089, val loss 2.7839\n",
      "Time : 14.191849 seconds\n",
      "step 499: train loss 2.7256, val loss 2.7723\n",
      "Time : 14.108956 seconds\n",
      "step 500: train loss 2.7182, val loss 2.7827\n",
      "Time : 14.112661 seconds\n",
      "step 501: train loss 2.7118, val loss 2.7798\n",
      "Time : 14.272386 seconds\n",
      "step 502: train loss 2.7120, val loss 2.7820\n",
      "Time : 14.310970 seconds\n",
      "step 503: train loss 2.7175, val loss 2.7783\n",
      "Time : 14.165574 seconds\n",
      "step 504: train loss 2.7140, val loss 2.7870\n",
      "Time : 14.128616 seconds\n",
      "step 505: train loss 2.7189, val loss 2.7809\n",
      "Time : 14.147794 seconds\n",
      "step 506: train loss 2.7207, val loss 2.7751\n",
      "Time : 14.135731 seconds\n",
      "step 507: train loss 2.7083, val loss 2.7818\n",
      "Time : 14.137698 seconds\n",
      "step 508: train loss 2.7116, val loss 2.7818\n",
      "Time : 14.154589 seconds\n",
      "step 509: train loss 2.7122, val loss 2.7785\n",
      "Time : 14.210658 seconds\n",
      "step 510: train loss 2.7182, val loss 2.7778\n",
      "Time : 14.292185 seconds\n",
      "step 511: train loss 2.7094, val loss 2.7786\n",
      "Time : 14.267978 seconds\n",
      "step 512: train loss 2.7160, val loss 2.7726\n",
      "Time : 14.108016 seconds\n",
      "step 513: train loss 2.7148, val loss 2.7725\n",
      "Time : 14.120752 seconds\n",
      "step 514: train loss 2.7159, val loss 2.7825\n",
      "Time : 14.159278 seconds\n",
      "step 515: train loss 2.7136, val loss 2.7734\n",
      "Time : 14.151217 seconds\n",
      "step 516: train loss 2.7133, val loss 2.7760\n",
      "Time : 14.104801 seconds\n",
      "step 517: train loss 2.7075, val loss 2.7724\n",
      "Time : 14.096630 seconds\n",
      "step 518: train loss 2.7082, val loss 2.7798\n",
      "Time : 14.337334 seconds\n",
      "step 519: train loss 2.7172, val loss 2.7806\n",
      "Time : 14.356317 seconds\n",
      "step 520: train loss 2.7096, val loss 2.7794\n",
      "Time : 14.153472 seconds\n",
      "step 521: train loss 2.7050, val loss 2.7783\n",
      "Time : 14.112809 seconds\n",
      "step 522: train loss 2.7135, val loss 2.7785\n",
      "Time : 14.180833 seconds\n",
      "step 523: train loss 2.7024, val loss 2.7813\n",
      "Time : 14.174626 seconds\n",
      "step 524: train loss 2.7101, val loss 2.7849\n",
      "Time : 14.156598 seconds\n",
      "step 525: train loss 2.7096, val loss 2.7836\n",
      "Time : 14.112607 seconds\n",
      "step 526: train loss 2.7126, val loss 2.7758\n",
      "Time : 14.231378 seconds\n",
      "step 527: train loss 2.7101, val loss 2.7737\n",
      "Time : 14.328056 seconds\n",
      "step 528: train loss 2.7119, val loss 2.7739\n",
      "Time : 14.232726 seconds\n",
      "step 529: train loss 2.7139, val loss 2.7826\n",
      "Time : 14.118185 seconds\n",
      "step 530: train loss 2.7082, val loss 2.7862\n",
      "Time : 14.107967 seconds\n",
      "step 531: train loss 2.7133, val loss 2.7814\n",
      "Time : 14.161360 seconds\n",
      "step 532: train loss 2.7113, val loss 2.7804\n",
      "Time : 14.166193 seconds\n",
      "step 533: train loss 2.7103, val loss 2.7806\n",
      "Time : 14.134930 seconds\n",
      "step 534: train loss 2.7063, val loss 2.7789\n",
      "Time : 14.140386 seconds\n",
      "step 535: train loss 2.7162, val loss 2.7726\n",
      "Time : 14.301839 seconds\n",
      "step 536: train loss 2.7071, val loss 2.7770\n",
      "Time : 14.318596 seconds\n",
      "step 537: train loss 2.7088, val loss 2.7814\n",
      "Time : 14.140688 seconds\n",
      "step 538: train loss 2.7101, val loss 2.7790\n",
      "Time : 14.147614 seconds\n",
      "step 539: train loss 2.7159, val loss 2.7706\n",
      "Time : 14.160671 seconds\n",
      "step 540: train loss 2.7131, val loss 2.7724\n",
      "Time : 14.188510 seconds\n",
      "step 541: train loss 2.7091, val loss 2.7760\n",
      "Time : 14.115481 seconds\n",
      "step 542: train loss 2.7127, val loss 2.7767\n",
      "Time : 14.126435 seconds\n",
      "step 543: train loss 2.7177, val loss 2.7820\n",
      "Time : 14.267025 seconds\n",
      "step 544: train loss 2.7138, val loss 2.7830\n",
      "Time : 14.345901 seconds\n",
      "step 545: train loss 2.7139, val loss 2.7744\n",
      "Time : 14.240971 seconds\n",
      "step 546: train loss 2.7069, val loss 2.7744\n",
      "Time : 14.114801 seconds\n",
      "step 547: train loss 2.7081, val loss 2.7670\n",
      "Time : 14.134363 seconds\n",
      "step 548: train loss 2.7046, val loss 2.7725\n",
      "Time : 14.184977 seconds\n",
      "step 549: train loss 2.7063, val loss 2.7722\n",
      "Time : 14.195000 seconds\n",
      "step 550: train loss 2.7087, val loss 2.7688\n",
      "Time : 14.113314 seconds\n",
      "step 551: train loss 2.7132, val loss 2.7749\n",
      "Time : 14.147744 seconds\n",
      "step 552: train loss 2.7120, val loss 2.7784\n",
      "Time : 14.349531 seconds\n",
      "step 553: train loss 2.7011, val loss 2.7789\n",
      "Time : 14.298753 seconds\n",
      "step 554: train loss 2.7142, val loss 2.7795\n",
      "Time : 14.134933 seconds\n",
      "step 555: train loss 2.7157, val loss 2.7721\n",
      "Time : 14.112511 seconds\n",
      "step 556: train loss 2.7070, val loss 2.7781\n",
      "Time : 14.133181 seconds\n",
      "step 557: train loss 2.7099, val loss 2.7742\n",
      "Time : 14.138873 seconds\n",
      "step 558: train loss 2.7077, val loss 2.7763\n",
      "Time : 14.140071 seconds\n",
      "step 559: train loss 2.7107, val loss 2.7751\n",
      "Time : 14.163343 seconds\n",
      "step 560: train loss 2.7027, val loss 2.7683\n",
      "Time : 14.268326 seconds\n",
      "step 561: train loss 2.7037, val loss 2.7664\n",
      "Time : 14.307207 seconds\n",
      "step 562: train loss 2.7078, val loss 2.7723\n",
      "Time : 14.211986 seconds\n",
      "step 563: train loss 2.7054, val loss 2.7714\n",
      "Time : 14.116843 seconds\n",
      "step 564: train loss 2.7054, val loss 2.7796\n",
      "Time : 14.143623 seconds\n",
      "step 565: train loss 2.7072, val loss 2.7706\n",
      "Time : 14.164779 seconds\n",
      "step 566: train loss 2.7148, val loss 2.7807\n",
      "Time : 14.150270 seconds\n",
      "step 567: train loss 2.7098, val loss 2.7737\n",
      "Time : 14.100202 seconds\n",
      "step 568: train loss 2.7055, val loss 2.7698\n",
      "Time : 14.181225 seconds\n",
      "step 569: train loss 2.7091, val loss 2.7777\n",
      "Time : 14.349444 seconds\n",
      "step 570: train loss 2.7142, val loss 2.7770\n",
      "Time : 14.332692 seconds\n",
      "step 571: train loss 2.7087, val loss 2.7717\n",
      "Time : 14.091792 seconds\n",
      "step 572: train loss 2.7057, val loss 2.7786\n",
      "Time : 14.098840 seconds\n",
      "step 573: train loss 2.7093, val loss 2.7756\n",
      "Time : 14.156130 seconds\n",
      "step 574: train loss 2.7057, val loss 2.7771\n",
      "Time : 14.176701 seconds\n",
      "step 575: train loss 2.7075, val loss 2.7795\n",
      "Time : 14.119388 seconds\n",
      "step 576: train loss 2.7068, val loss 2.7764\n",
      "Time : 14.101314 seconds\n",
      "step 577: train loss 2.7105, val loss 2.7746\n",
      "Time : 14.258828 seconds\n",
      "step 578: train loss 2.7115, val loss 2.7741\n",
      "Time : 14.321814 seconds\n",
      "step 579: train loss 2.7077, val loss 2.7742\n",
      "Time : 14.174975 seconds\n",
      "step 580: train loss 2.7078, val loss 2.7747\n",
      "Time : 14.105950 seconds\n",
      "step 581: train loss 2.7054, val loss 2.7756\n",
      "Time : 14.138008 seconds\n",
      "step 582: train loss 2.7105, val loss 2.7796\n",
      "Time : 14.140332 seconds\n",
      "step 583: train loss 2.7100, val loss 2.7722\n",
      "Time : 14.140022 seconds\n",
      "step 584: train loss 2.7062, val loss 2.7763\n",
      "Time : 14.156973 seconds\n",
      "step 585: train loss 2.7102, val loss 2.7753\n",
      "Time : 14.176847 seconds\n",
      "step 586: train loss 2.7044, val loss 2.7795\n",
      "Time : 14.304996 seconds\n",
      "step 587: train loss 2.7116, val loss 2.7754\n",
      "Time : 14.263830 seconds\n",
      "step 588: train loss 2.7082, val loss 2.7704\n",
      "Time : 14.130183 seconds\n",
      "step 589: train loss 2.7073, val loss 2.7737\n",
      "Time : 14.143797 seconds\n",
      "step 590: train loss 2.7059, val loss 2.7700\n",
      "Time : 14.186077 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 591: train loss 2.7037, val loss 2.7635\n",
      "Time : 14.181374 seconds\n",
      "step 592: train loss 2.7059, val loss 2.7621\n",
      "Time : 14.114610 seconds\n",
      "step 593: train loss 2.7011, val loss 2.7722\n",
      "Time : 14.114621 seconds\n",
      "step 594: train loss 2.6980, val loss 2.7690\n",
      "Time : 14.328017 seconds\n",
      "step 595: train loss 2.7091, val loss 2.7793\n",
      "Time : 14.357171 seconds\n",
      "step 596: train loss 2.7181, val loss 2.7730\n",
      "Time : 14.159525 seconds\n",
      "step 597: train loss 2.7129, val loss 2.7700\n",
      "Time : 14.096157 seconds\n",
      "step 598: train loss 2.7149, val loss 2.7704\n",
      "Time : 14.139545 seconds\n",
      "step 599: train loss 2.7024, val loss 2.7797\n",
      "Time : 14.190609 seconds\n",
      "step 600: train loss 2.7136, val loss 2.7616\n",
      "Time : 14.132528 seconds\n",
      "step 601: train loss 2.7091, val loss 2.7775\n",
      "Time : 14.105621 seconds\n",
      "step 602: train loss 2.7092, val loss 2.7776\n",
      "Time : 14.198645 seconds\n",
      "step 603: train loss 2.7057, val loss 2.7704\n",
      "Time : 14.321533 seconds\n",
      "step 604: train loss 2.6948, val loss 2.7700\n",
      "Time : 14.256386 seconds\n",
      "step 605: train loss 2.6997, val loss 2.7683\n",
      "Time : 14.124439 seconds\n",
      "step 606: train loss 2.7055, val loss 2.7706\n",
      "Time : 14.108506 seconds\n",
      "step 607: train loss 2.7045, val loss 2.7688\n",
      "Time : 14.153647 seconds\n",
      "step 608: train loss 2.7046, val loss 2.7788\n",
      "Time : 14.144901 seconds\n",
      "step 609: train loss 2.7096, val loss 2.7720\n",
      "Time : 14.118784 seconds\n",
      "step 610: train loss 2.7039, val loss 2.7773\n",
      "Time : 14.137190 seconds\n",
      "step 611: train loss 2.6969, val loss 2.7702\n",
      "Time : 14.278733 seconds\n",
      "step 612: train loss 2.7066, val loss 2.7763\n",
      "Time : 14.287451 seconds\n",
      "step 613: train loss 2.7086, val loss 2.7768\n",
      "Time : 14.171539 seconds\n",
      "step 614: train loss 2.7083, val loss 2.7729\n",
      "Time : 14.128708 seconds\n",
      "step 615: train loss 2.7032, val loss 2.7665\n",
      "Time : 14.144167 seconds\n",
      "step 616: train loss 2.6951, val loss 2.7662\n",
      "Time : 14.162501 seconds\n",
      "step 617: train loss 2.6999, val loss 2.7696\n",
      "Time : 14.144368 seconds\n",
      "step 618: train loss 2.6987, val loss 2.7657\n",
      "Time : 14.090299 seconds\n",
      "step 619: train loss 2.6995, val loss 2.7764\n",
      "Time : 14.218631 seconds\n",
      "step 620: train loss 2.7065, val loss 2.7704\n",
      "Time : 14.381944 seconds\n",
      "step 621: train loss 2.7028, val loss 2.7677\n",
      "Time : 14.258111 seconds\n",
      "step 622: train loss 2.7033, val loss 2.7725\n",
      "Time : 14.097061 seconds\n",
      "step 623: train loss 2.7074, val loss 2.7721\n",
      "Time : 14.111209 seconds\n",
      "step 624: train loss 2.7079, val loss 2.7701\n",
      "Time : 14.162840 seconds\n",
      "step 625: train loss 2.6988, val loss 2.7676\n",
      "Time : 14.156443 seconds\n",
      "step 626: train loss 2.7043, val loss 2.7686\n",
      "Time : 14.105330 seconds\n",
      "step 627: train loss 2.7038, val loss 2.7739\n",
      "Time : 14.104921 seconds\n",
      "step 628: train loss 2.7027, val loss 2.7749\n",
      "Time : 14.291985 seconds\n",
      "step 629: train loss 2.6979, val loss 2.7673\n",
      "Time : 14.298901 seconds\n",
      "step 630: train loss 2.6997, val loss 2.7689\n",
      "Time : 14.152634 seconds\n",
      "step 631: train loss 2.7031, val loss 2.7685\n",
      "Time : 14.117566 seconds\n",
      "step 632: train loss 2.7020, val loss 2.7704\n",
      "Time : 14.144199 seconds\n",
      "step 633: train loss 2.7037, val loss 2.7674\n",
      "Time : 14.118611 seconds\n",
      "step 634: train loss 2.7083, val loss 2.7656\n",
      "Time : 14.143080 seconds\n",
      "step 635: train loss 2.7005, val loss 2.7677\n",
      "Time : 14.133275 seconds\n",
      "step 636: train loss 2.6978, val loss 2.7677\n",
      "Time : 14.228369 seconds\n",
      "step 637: train loss 2.6986, val loss 2.7644\n",
      "Time : 14.276699 seconds\n",
      "step 638: train loss 2.7018, val loss 2.7655\n",
      "Time : 14.238258 seconds\n",
      "step 639: train loss 2.7036, val loss 2.7706\n",
      "Time : 14.131124 seconds\n",
      "step 640: train loss 2.7008, val loss 2.7695\n",
      "Time : 14.158401 seconds\n",
      "step 641: train loss 2.7019, val loss 2.7729\n",
      "Time : 14.179866 seconds\n",
      "step 642: train loss 2.6996, val loss 2.7739\n",
      "Time : 14.157382 seconds\n",
      "step 643: train loss 2.7111, val loss 2.7696\n",
      "Time : 14.105014 seconds\n",
      "step 644: train loss 2.7048, val loss 2.7757\n",
      "Time : 14.117966 seconds\n",
      "step 645: train loss 2.6960, val loss 2.7634\n",
      "Time : 14.366174 seconds\n",
      "step 646: train loss 2.6972, val loss 2.7618\n",
      "Time : 14.333758 seconds\n",
      "step 647: train loss 2.7037, val loss 2.7736\n",
      "Time : 14.113769 seconds\n",
      "step 648: train loss 2.7036, val loss 2.7598\n",
      "Time : 14.113640 seconds\n",
      "step 649: train loss 2.7037, val loss 2.7646\n",
      "Time : 14.153250 seconds\n",
      "step 650: train loss 2.6987, val loss 2.7711\n",
      "Time : 14.196813 seconds\n",
      "step 651: train loss 2.7102, val loss 2.7672\n",
      "Time : 14.150196 seconds\n",
      "step 652: train loss 2.6938, val loss 2.7616\n",
      "Time : 14.089619 seconds\n",
      "step 653: train loss 2.6972, val loss 2.7722\n",
      "Time : 14.228860 seconds\n",
      "step 654: train loss 2.6988, val loss 2.7684\n",
      "Time : 14.333016 seconds\n",
      "step 655: train loss 2.7010, val loss 2.7695\n",
      "Time : 14.278382 seconds\n",
      "step 656: train loss 2.7020, val loss 2.7660\n",
      "Time : 14.594585 seconds\n",
      "step 657: train loss 2.6970, val loss 2.7652\n",
      "Time : 14.436338 seconds\n",
      "step 658: train loss 2.7057, val loss 2.7693\n",
      "Time : 14.578237 seconds\n",
      "step 659: train loss 2.6967, val loss 2.7712\n",
      "Time : 14.181774 seconds\n",
      "step 660: train loss 2.7008, val loss 2.7717\n",
      "Time : 14.182984 seconds\n",
      "step 661: train loss 2.6997, val loss 2.7644\n",
      "Time : 14.203025 seconds\n",
      "step 662: train loss 2.6990, val loss 2.7633\n",
      "Time : 14.303859 seconds\n",
      "step 663: train loss 2.6951, val loss 2.7634\n",
      "Time : 14.290099 seconds\n",
      "step 664: train loss 2.6984, val loss 2.7726\n",
      "Time : 14.144813 seconds\n",
      "step 665: train loss 2.7059, val loss 2.7690\n",
      "Time : 14.164713 seconds\n",
      "step 666: train loss 2.6954, val loss 2.7679\n",
      "Time : 14.219460 seconds\n",
      "step 667: train loss 2.6983, val loss 2.7734\n",
      "Time : 14.183224 seconds\n",
      "step 668: train loss 2.6979, val loss 2.7677\n",
      "Time : 14.135167 seconds\n",
      "step 669: train loss 2.6924, val loss 2.7651\n",
      "Time : 14.120494 seconds\n",
      "step 670: train loss 2.6982, val loss 2.7750\n",
      "Time : 14.321912 seconds\n",
      "step 671: train loss 2.7009, val loss 2.7718\n",
      "Time : 14.377002 seconds\n",
      "step 672: train loss 2.7000, val loss 2.7725\n",
      "Time : 14.230013 seconds\n",
      "step 673: train loss 2.7099, val loss 2.7700\n",
      "Time : 14.142766 seconds\n",
      "step 674: train loss 2.6989, val loss 2.7695\n",
      "Time : 14.174053 seconds\n",
      "step 675: train loss 2.7016, val loss 2.7760\n",
      "Time : 14.212627 seconds\n",
      "step 676: train loss 2.6998, val loss 2.7717\n",
      "Time : 14.157553 seconds\n",
      "step 677: train loss 2.6961, val loss 2.7706\n",
      "Time : 14.097685 seconds\n",
      "step 678: train loss 2.6955, val loss 2.7725\n",
      "Time : 14.196788 seconds\n",
      "step 679: train loss 2.7051, val loss 2.7703\n",
      "Time : 14.334801 seconds\n",
      "step 680: train loss 2.6978, val loss 2.7652\n",
      "Time : 14.288842 seconds\n",
      "step 681: train loss 2.6975, val loss 2.7629\n",
      "Time : 14.136641 seconds\n",
      "step 682: train loss 2.7054, val loss 2.7663\n",
      "Time : 14.108692 seconds\n",
      "step 683: train loss 2.7024, val loss 2.7743\n",
      "Time : 14.139310 seconds\n",
      "step 684: train loss 2.6990, val loss 2.7663\n",
      "Time : 14.167231 seconds\n",
      "step 685: train loss 2.6932, val loss 2.7610\n",
      "Time : 14.170815 seconds\n",
      "step 686: train loss 2.6961, val loss 2.7664\n",
      "Time : 14.157082 seconds\n",
      "step 687: train loss 2.6909, val loss 2.7646\n",
      "Time : 14.291573 seconds\n",
      "step 688: train loss 2.6956, val loss 2.7704\n",
      "Time : 14.280548 seconds\n",
      "step 689: train loss 2.6968, val loss 2.7595\n",
      "Time : 14.152491 seconds\n",
      "step 690: train loss 2.7057, val loss 2.7627\n",
      "Time : 14.118044 seconds\n",
      "step 691: train loss 2.7035, val loss 2.7671\n",
      "Time : 14.152263 seconds\n",
      "step 692: train loss 2.7016, val loss 2.7734\n",
      "Time : 14.169714 seconds\n",
      "step 693: train loss 2.7029, val loss 2.7683\n",
      "Time : 14.169462 seconds\n",
      "step 694: train loss 2.7013, val loss 2.7703\n",
      "Time : 14.195681 seconds\n",
      "step 695: train loss 2.7003, val loss 2.7645\n",
      "Time : 14.215414 seconds\n",
      "step 696: train loss 2.6988, val loss 2.7610\n",
      "Time : 14.361604 seconds\n",
      "step 697: train loss 2.6986, val loss 2.7667\n",
      "Time : 14.288178 seconds\n",
      "step 698: train loss 2.6960, val loss 2.7723\n",
      "Time : 14.113156 seconds\n",
      "step 699: train loss 2.6979, val loss 2.7658\n",
      "Time : 14.145515 seconds\n",
      "step 700: train loss 2.6998, val loss 2.7726\n",
      "Time : 14.197534 seconds\n",
      "step 701: train loss 2.6914, val loss 2.7670\n",
      "Time : 14.170980 seconds\n",
      "step 702: train loss 2.6956, val loss 2.7695\n",
      "Time : 14.116345 seconds\n",
      "step 703: train loss 2.7033, val loss 2.7710\n",
      "Time : 14.123424 seconds\n",
      "step 704: train loss 2.7078, val loss 2.7700\n",
      "Time : 14.322212 seconds\n",
      "step 705: train loss 2.7040, val loss 2.7677\n",
      "Time : 14.334902 seconds\n",
      "step 706: train loss 2.6957, val loss 2.7544\n",
      "Time : 14.175760 seconds\n",
      "step 707: train loss 2.7038, val loss 2.7673\n",
      "Time : 14.130848 seconds\n",
      "step 708: train loss 2.7036, val loss 2.7674\n",
      "Time : 14.157470 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 709: train loss 2.6974, val loss 2.7637\n",
      "Time : 14.143538 seconds\n",
      "step 710: train loss 2.7003, val loss 2.7719\n",
      "Time : 14.123603 seconds\n",
      "step 711: train loss 2.6981, val loss 2.7656\n",
      "Time : 14.139292 seconds\n",
      "step 712: train loss 2.7032, val loss 2.7693\n",
      "Time : 14.203907 seconds\n",
      "step 713: train loss 2.7009, val loss 2.7671\n",
      "Time : 14.275081 seconds\n",
      "step 714: train loss 2.6994, val loss 2.7579\n",
      "Time : 14.243869 seconds\n",
      "step 715: train loss 2.6916, val loss 2.7671\n",
      "Time : 14.163043 seconds\n",
      "step 716: train loss 2.6885, val loss 2.7599\n",
      "Time : 14.144562 seconds\n",
      "step 717: train loss 2.6998, val loss 2.7638\n",
      "Time : 14.175021 seconds\n",
      "step 718: train loss 2.6955, val loss 2.7631\n",
      "Time : 14.158701 seconds\n",
      "step 719: train loss 2.7016, val loss 2.7745\n",
      "Time : 14.115753 seconds\n",
      "step 720: train loss 2.6949, val loss 2.7687\n",
      "Time : 14.142457 seconds\n",
      "step 721: train loss 2.6974, val loss 2.7639\n",
      "Time : 14.384966 seconds\n",
      "step 722: train loss 2.6962, val loss 2.7651\n",
      "Time : 14.369500 seconds\n",
      "step 723: train loss 2.6897, val loss 2.7588\n",
      "Time : 14.117646 seconds\n",
      "step 724: train loss 2.6934, val loss 2.7628\n",
      "Time : 14.120864 seconds\n",
      "step 725: train loss 2.6991, val loss 2.7557\n",
      "Time : 14.157826 seconds\n",
      "step 726: train loss 2.6930, val loss 2.7616\n",
      "Time : 14.164966 seconds\n",
      "step 727: train loss 2.6960, val loss 2.7679\n",
      "Time : 14.133711 seconds\n",
      "step 728: train loss 2.6919, val loss 2.7706\n",
      "Time : 14.101952 seconds\n",
      "step 729: train loss 2.6893, val loss 2.7697\n",
      "Time : 14.207084 seconds\n",
      "step 730: train loss 2.6881, val loss 2.7599\n",
      "Time : 14.319974 seconds\n",
      "step 731: train loss 2.6988, val loss 2.7621\n",
      "Time : 14.239668 seconds\n",
      "step 732: train loss 2.6941, val loss 2.7728\n",
      "Time : 14.122120 seconds\n",
      "step 733: train loss 2.6961, val loss 2.7590\n",
      "Time : 14.107481 seconds\n",
      "step 734: train loss 2.6915, val loss 2.7676\n",
      "Time : 14.163629 seconds\n",
      "step 735: train loss 2.6921, val loss 2.7628\n",
      "Time : 14.140488 seconds\n",
      "step 736: train loss 2.6931, val loss 2.7653\n",
      "Time : 14.151123 seconds\n",
      "step 737: train loss 2.6991, val loss 2.7651\n",
      "Time : 14.176122 seconds\n",
      "step 738: train loss 2.6960, val loss 2.7644\n",
      "Time : 14.300103 seconds\n",
      "step 739: train loss 2.6992, val loss 2.7695\n",
      "Time : 14.341886 seconds\n",
      "step 740: train loss 2.6923, val loss 2.7683\n",
      "Time : 14.156093 seconds\n",
      "step 741: train loss 2.6919, val loss 2.7687\n",
      "Time : 14.151018 seconds\n",
      "step 742: train loss 2.6898, val loss 2.7561\n",
      "Time : 14.164882 seconds\n",
      "step 743: train loss 2.6925, val loss 2.7647\n",
      "Time : 14.172480 seconds\n",
      "step 744: train loss 2.6989, val loss 2.7615\n",
      "Time : 14.129519 seconds\n",
      "step 745: train loss 2.6915, val loss 2.7676\n",
      "Time : 14.129933 seconds\n",
      "step 746: train loss 2.6892, val loss 2.7633\n",
      "Time : 14.265497 seconds\n",
      "step 747: train loss 2.6961, val loss 2.7626\n",
      "Time : 14.353526 seconds\n",
      "step 748: train loss 2.6809, val loss 2.7658\n",
      "Time : 14.214527 seconds\n",
      "step 749: train loss 2.6931, val loss 2.7662\n",
      "Time : 14.113829 seconds\n",
      "step 750: train loss 2.6941, val loss 2.7768\n",
      "Time : 14.155528 seconds\n",
      "step 751: train loss 2.6918, val loss 2.7642\n",
      "Time : 14.194902 seconds\n",
      "step 752: train loss 2.7012, val loss 2.7641\n",
      "Time : 14.167110 seconds\n",
      "step 753: train loss 2.6940, val loss 2.7548\n",
      "Time : 14.105780 seconds\n",
      "step 754: train loss 2.6935, val loss 2.7576\n",
      "Time : 14.154226 seconds\n",
      "step 755: train loss 2.6964, val loss 2.7645\n",
      "Time : 14.333751 seconds\n",
      "step 756: train loss 2.6940, val loss 2.7595\n",
      "Time : 14.294317 seconds\n",
      "step 757: train loss 2.6948, val loss 2.7629\n",
      "Time : 14.119125 seconds\n",
      "step 758: train loss 2.6975, val loss 2.7578\n",
      "Time : 14.132154 seconds\n",
      "step 759: train loss 2.6898, val loss 2.7565\n",
      "Time : 14.170172 seconds\n",
      "step 760: train loss 2.6899, val loss 2.7585\n",
      "Time : 14.135170 seconds\n",
      "step 761: train loss 2.6878, val loss 2.7550\n",
      "Time : 14.164334 seconds\n",
      "step 762: train loss 2.6936, val loss 2.7589\n",
      "Time : 14.171769 seconds\n",
      "step 763: train loss 2.6919, val loss 2.7619\n",
      "Time : 14.247233 seconds\n",
      "step 764: train loss 2.6878, val loss 2.7660\n",
      "Time : 14.263901 seconds\n",
      "step 765: train loss 2.6950, val loss 2.7649\n",
      "Time : 14.198380 seconds\n",
      "step 766: train loss 2.6916, val loss 2.7581\n",
      "Time : 14.118291 seconds\n",
      "step 767: train loss 2.6956, val loss 2.7555\n",
      "Time : 14.156155 seconds\n",
      "step 768: train loss 2.6940, val loss 2.7559\n",
      "Time : 14.129724 seconds\n",
      "step 769: train loss 2.6904, val loss 2.7597\n",
      "Time : 14.151893 seconds\n",
      "step 770: train loss 2.6881, val loss 2.7617\n",
      "Time : 14.095274 seconds\n",
      "step 771: train loss 2.6911, val loss 2.7603\n",
      "Time : 14.194058 seconds\n",
      "step 772: train loss 2.6926, val loss 2.7634\n",
      "Time : 14.368331 seconds\n",
      "step 773: train loss 2.6912, val loss 2.7659\n",
      "Time : 14.298858 seconds\n",
      "step 774: train loss 2.6911, val loss 2.7572\n",
      "Time : 14.115615 seconds\n",
      "step 775: train loss 2.6884, val loss 2.7578\n",
      "Time : 14.121892 seconds\n",
      "step 776: train loss 2.6908, val loss 2.7525\n",
      "Time : 14.194162 seconds\n",
      "step 777: train loss 2.6905, val loss 2.7559\n",
      "Time : 14.205095 seconds\n",
      "step 778: train loss 2.6852, val loss 2.7588\n",
      "Time : 14.133198 seconds\n",
      "step 779: train loss 2.6898, val loss 2.7578\n",
      "Time : 14.114937 seconds\n",
      "step 780: train loss 2.6969, val loss 2.7562\n",
      "Time : 14.293580 seconds\n",
      "step 781: train loss 2.6943, val loss 2.7578\n",
      "Time : 14.323275 seconds\n",
      "step 782: train loss 2.6945, val loss 2.7606\n",
      "Time : 14.171995 seconds\n",
      "step 783: train loss 2.6874, val loss 2.7577\n",
      "Time : 14.115914 seconds\n",
      "step 784: train loss 2.6907, val loss 2.7586\n",
      "Time : 14.121993 seconds\n",
      "step 785: train loss 2.6958, val loss 2.7606\n",
      "Time : 14.142604 seconds\n",
      "step 786: train loss 2.6951, val loss 2.7602\n",
      "Time : 14.145304 seconds\n",
      "step 787: train loss 2.6978, val loss 2.7596\n",
      "Time : 14.136844 seconds\n",
      "step 788: train loss 2.6911, val loss 2.7613\n",
      "Time : 14.214341 seconds\n",
      "step 789: train loss 2.6921, val loss 2.7639\n",
      "Time : 14.304136 seconds\n",
      "step 790: train loss 2.6966, val loss 2.7588\n",
      "Time : 14.249918 seconds\n",
      "step 791: train loss 2.6905, val loss 2.7649\n",
      "Time : 14.116540 seconds\n",
      "step 792: train loss 2.6924, val loss 2.7620\n",
      "Time : 14.139107 seconds\n",
      "step 793: train loss 2.6850, val loss 2.7606\n",
      "Time : 14.164608 seconds\n",
      "step 794: train loss 2.6933, val loss 2.7651\n",
      "Time : 14.168655 seconds\n",
      "step 795: train loss 2.6910, val loss 2.7573\n",
      "Time : 14.108823 seconds\n",
      "step 796: train loss 2.6870, val loss 2.7550\n",
      "Time : 14.108714 seconds\n",
      "step 797: train loss 2.6888, val loss 2.7558\n",
      "Time : 14.338618 seconds\n",
      "step 798: train loss 2.6894, val loss 2.7579\n",
      "Time : 14.341125 seconds\n",
      "step 799: train loss 2.6937, val loss 2.7688\n",
      "Time : 14.147468 seconds\n",
      "step 800: train loss 2.6900, val loss 2.7597\n",
      "Time : 14.104258 seconds\n",
      "step 801: train loss 2.6925, val loss 2.7555\n",
      "Time : 14.168261 seconds\n",
      "step 802: train loss 2.6892, val loss 2.7613\n",
      "Time : 14.139189 seconds\n",
      "step 803: train loss 2.6908, val loss 2.7593\n",
      "Time : 14.132714 seconds\n",
      "step 804: train loss 2.6904, val loss 2.7588\n",
      "Time : 14.099891 seconds\n",
      "step 805: train loss 2.6894, val loss 2.7627\n",
      "Time : 14.199497 seconds\n",
      "step 806: train loss 2.6969, val loss 2.7600\n",
      "Time : 14.297647 seconds\n",
      "step 807: train loss 2.6887, val loss 2.7603\n",
      "Time : 14.270582 seconds\n",
      "step 808: train loss 2.6891, val loss 2.7628\n",
      "Time : 14.124262 seconds\n",
      "step 809: train loss 2.6869, val loss 2.7518\n",
      "Time : 14.121898 seconds\n",
      "step 810: train loss 2.6838, val loss 2.7524\n",
      "Time : 14.145758 seconds\n",
      "step 811: train loss 2.6861, val loss 2.7575\n",
      "Time : 14.167825 seconds\n",
      "step 812: train loss 2.6939, val loss 2.7641\n",
      "Time : 14.162928 seconds\n",
      "step 813: train loss 2.6874, val loss 2.7613\n",
      "Time : 14.143573 seconds\n",
      "step 814: train loss 2.6870, val loss 2.7652\n",
      "Time : 14.270791 seconds\n",
      "step 815: train loss 2.6861, val loss 2.7575\n",
      "Time : 14.223650 seconds\n",
      "step 816: train loss 2.6842, val loss 2.7608\n",
      "Time : 14.133596 seconds\n",
      "step 817: train loss 2.6938, val loss 2.7565\n",
      "Time : 14.120436 seconds\n",
      "step 818: train loss 2.6816, val loss 2.7610\n",
      "Time : 14.140968 seconds\n",
      "step 819: train loss 2.6871, val loss 2.7546\n",
      "Time : 14.163371 seconds\n",
      "step 820: train loss 2.6894, val loss 2.7550\n",
      "Time : 14.200424 seconds\n",
      "step 821: train loss 2.6834, val loss 2.7555\n",
      "Time : 14.149981 seconds\n",
      "step 822: train loss 2.6818, val loss 2.7603\n",
      "Time : 14.252905 seconds\n",
      "step 823: train loss 2.6896, val loss 2.7589\n",
      "Time : 14.340307 seconds\n",
      "step 824: train loss 2.6895, val loss 2.7574\n",
      "Time : 14.256441 seconds\n",
      "step 825: train loss 2.6802, val loss 2.7630\n",
      "Time : 14.101983 seconds\n",
      "step 826: train loss 2.6858, val loss 2.7525\n",
      "Time : 14.117683 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 827: train loss 2.6871, val loss 2.7522\n",
      "Time : 14.183056 seconds\n",
      "step 828: train loss 2.6888, val loss 2.7544\n",
      "Time : 14.154985 seconds\n",
      "step 829: train loss 2.6859, val loss 2.7620\n",
      "Time : 14.112700 seconds\n",
      "step 830: train loss 2.6850, val loss 2.7543\n",
      "Time : 14.109104 seconds\n",
      "step 831: train loss 2.6849, val loss 2.7518\n",
      "Time : 14.302812 seconds\n",
      "step 832: train loss 2.6801, val loss 2.7587\n",
      "Time : 14.304141 seconds\n",
      "step 833: train loss 2.6923, val loss 2.7556\n",
      "Time : 14.126079 seconds\n",
      "step 834: train loss 2.6894, val loss 2.7651\n",
      "Time : 14.123835 seconds\n",
      "step 835: train loss 2.6879, val loss 2.7594\n",
      "Time : 14.144480 seconds\n",
      "step 836: train loss 2.6846, val loss 2.7590\n",
      "Time : 14.134665 seconds\n",
      "step 837: train loss 2.6935, val loss 2.7560\n",
      "Time : 14.146576 seconds\n",
      "step 838: train loss 2.6884, val loss 2.7510\n",
      "Time : 14.147535 seconds\n",
      "step 839: train loss 2.6913, val loss 2.7499\n",
      "Time : 14.234916 seconds\n",
      "step 840: train loss 2.6854, val loss 2.7521\n",
      "Time : 14.286472 seconds\n",
      "step 841: train loss 2.6815, val loss 2.7601\n",
      "Time : 14.200364 seconds\n",
      "step 842: train loss 2.6895, val loss 2.7532\n",
      "Time : 14.127729 seconds\n",
      "step 843: train loss 2.6854, val loss 2.7683\n",
      "Time : 14.137574 seconds\n",
      "step 844: train loss 2.6953, val loss 2.7507\n",
      "Time : 14.174078 seconds\n",
      "step 845: train loss 2.6837, val loss 2.7530\n",
      "Time : 14.161304 seconds\n",
      "step 846: train loss 2.6906, val loss 2.7567\n",
      "Time : 14.123785 seconds\n",
      "step 847: train loss 2.6853, val loss 2.7595\n",
      "Time : 14.135448 seconds\n",
      "step 848: train loss 2.6813, val loss 2.7601\n",
      "Time : 14.346068 seconds\n",
      "step 849: train loss 2.6838, val loss 2.7614\n",
      "Time : 14.359615 seconds\n",
      "step 850: train loss 2.6891, val loss 2.7600\n",
      "Time : 14.096735 seconds\n",
      "step 851: train loss 2.6913, val loss 2.7580\n",
      "Time : 14.102124 seconds\n",
      "step 852: train loss 2.6876, val loss 2.7570\n",
      "Time : 14.178501 seconds\n",
      "step 853: train loss 2.6896, val loss 2.7615\n",
      "Time : 14.167903 seconds\n",
      "step 854: train loss 2.6849, val loss 2.7546\n",
      "Time : 14.111841 seconds\n",
      "step 855: train loss 2.6816, val loss 2.7553\n",
      "Time : 14.084810 seconds\n",
      "step 856: train loss 2.6846, val loss 2.7589\n",
      "Time : 14.237834 seconds\n",
      "step 857: train loss 2.6827, val loss 2.7514\n",
      "Time : 14.332294 seconds\n",
      "step 858: train loss 2.6935, val loss 2.7589\n",
      "Time : 14.203088 seconds\n",
      "step 859: train loss 2.6882, val loss 2.7618\n",
      "Time : 14.108384 seconds\n",
      "step 860: train loss 2.6849, val loss 2.7521\n",
      "Time : 14.121259 seconds\n",
      "step 861: train loss 2.6925, val loss 2.7675\n",
      "Time : 14.140906 seconds\n",
      "step 862: train loss 2.6874, val loss 2.7653\n",
      "Time : 14.143539 seconds\n",
      "step 863: train loss 2.6892, val loss 2.7586\n",
      "Time : 14.181813 seconds\n",
      "step 864: train loss 2.6923, val loss 2.7638\n",
      "Time : 14.184201 seconds\n",
      "step 865: train loss 2.6943, val loss 2.7643\n",
      "Time : 14.292644 seconds\n",
      "step 866: train loss 2.6910, val loss 2.7641\n",
      "Time : 14.275142 seconds\n",
      "step 867: train loss 2.6950, val loss 2.7573\n",
      "Time : 14.124380 seconds\n",
      "step 868: train loss 2.6903, val loss 2.7648\n",
      "Time : 14.131915 seconds\n",
      "step 869: train loss 2.6864, val loss 2.7635\n",
      "Time : 14.162274 seconds\n",
      "step 870: train loss 2.6857, val loss 2.7564\n",
      "Time : 14.170261 seconds\n",
      "step 871: train loss 2.6890, val loss 2.7548\n",
      "Time : 14.145212 seconds\n",
      "step 872: train loss 2.6947, val loss 2.7619\n",
      "Time : 14.115847 seconds\n",
      "step 873: train loss 2.6914, val loss 2.7582\n",
      "Time : 14.318467 seconds\n",
      "step 874: train loss 2.6875, val loss 2.7612\n",
      "Time : 14.366417 seconds\n",
      "step 875: train loss 2.6827, val loss 2.7516\n",
      "Time : 14.210417 seconds\n",
      "step 876: train loss 2.6912, val loss 2.7546\n",
      "Time : 14.113268 seconds\n",
      "step 877: train loss 2.6873, val loss 2.7632\n",
      "Time : 14.109546 seconds\n",
      "step 878: train loss 2.6862, val loss 2.7467\n",
      "Time : 14.193246 seconds\n",
      "step 879: train loss 2.6843, val loss 2.7543\n",
      "Time : 14.138433 seconds\n",
      "step 880: train loss 2.6917, val loss 2.7550\n",
      "Time : 14.107030 seconds\n",
      "step 881: train loss 2.6833, val loss 2.7561\n",
      "Time : 14.162925 seconds\n",
      "step 882: train loss 2.6857, val loss 2.7505\n",
      "Time : 14.303521 seconds\n",
      "step 883: train loss 2.6825, val loss 2.7529\n",
      "Time : 14.265070 seconds\n",
      "step 884: train loss 2.6870, val loss 2.7506\n",
      "Time : 14.118530 seconds\n",
      "step 885: train loss 2.6818, val loss 2.7538\n",
      "Time : 14.116993 seconds\n",
      "step 886: train loss 2.6828, val loss 2.7531\n",
      "Time : 14.157194 seconds\n",
      "step 887: train loss 2.6875, val loss 2.7517\n",
      "Time : 14.147372 seconds\n",
      "step 888: train loss 2.6834, val loss 2.7498\n",
      "Time : 14.173265 seconds\n",
      "step 889: train loss 2.6819, val loss 2.7495\n",
      "Time : 14.160166 seconds\n",
      "step 890: train loss 2.6871, val loss 2.7571\n",
      "Time : 14.254640 seconds\n",
      "step 891: train loss 2.6816, val loss 2.7539\n",
      "Time : 14.270723 seconds\n",
      "step 892: train loss 2.6813, val loss 2.7518\n",
      "Time : 14.158640 seconds\n",
      "step 893: train loss 2.6877, val loss 2.7443\n",
      "Time : 14.127067 seconds\n",
      "step 894: train loss 2.6911, val loss 2.7507\n",
      "Time : 14.137451 seconds\n",
      "step 895: train loss 2.6870, val loss 2.7456\n",
      "Time : 14.135451 seconds\n",
      "step 896: train loss 2.6926, val loss 2.7509\n",
      "Time : 14.133702 seconds\n",
      "step 897: train loss 2.6812, val loss 2.7570\n",
      "Time : 14.096990 seconds\n",
      "step 898: train loss 2.6837, val loss 2.7518\n",
      "Time : 14.183575 seconds\n",
      "step 899: train loss 2.6851, val loss 2.7485\n",
      "Time : 14.344474 seconds\n",
      "step 900: train loss 2.6866, val loss 2.7542\n",
      "Time : 14.282610 seconds\n",
      "step 901: train loss 2.6729, val loss 2.7469\n",
      "Time : 14.104846 seconds\n",
      "step 902: train loss 2.6821, val loss 2.7545\n",
      "Time : 14.123510 seconds\n",
      "step 903: train loss 2.6885, val loss 2.7529\n",
      "Time : 14.177394 seconds\n",
      "step 904: train loss 2.6803, val loss 2.7516\n",
      "Time : 14.152539 seconds\n",
      "step 905: train loss 2.6867, val loss 2.7567\n",
      "Time : 14.100294 seconds\n",
      "step 906: train loss 2.6837, val loss 2.7513\n",
      "Time : 14.099083 seconds\n",
      "step 907: train loss 2.6847, val loss 2.7558\n",
      "Time : 14.295581 seconds\n",
      "step 908: train loss 2.6831, val loss 2.7450\n",
      "Time : 14.328501 seconds\n",
      "step 909: train loss 2.6769, val loss 2.7501\n",
      "Time : 14.222694 seconds\n",
      "step 910: train loss 2.6767, val loss 2.7562\n",
      "Time : 14.124321 seconds\n",
      "step 911: train loss 2.6849, val loss 2.7509\n",
      "Time : 14.133801 seconds\n",
      "step 912: train loss 2.6857, val loss 2.7562\n",
      "Time : 14.150180 seconds\n",
      "step 913: train loss 2.6771, val loss 2.7498\n",
      "Time : 14.144859 seconds\n",
      "step 914: train loss 2.6795, val loss 2.7498\n",
      "Time : 14.149446 seconds\n",
      "step 915: train loss 2.6830, val loss 2.7501\n",
      "Time : 14.220033 seconds\n",
      "step 916: train loss 2.6819, val loss 2.7529\n",
      "Time : 14.312258 seconds\n",
      "step 917: train loss 2.6838, val loss 2.7543\n",
      "Time : 14.218575 seconds\n",
      "step 918: train loss 2.6762, val loss 2.7542\n",
      "Time : 14.132174 seconds\n",
      "step 919: train loss 2.6870, val loss 2.7571\n",
      "Time : 14.143788 seconds\n",
      "step 920: train loss 2.6831, val loss 2.7481\n",
      "Time : 14.190833 seconds\n",
      "step 921: train loss 2.6771, val loss 2.7581\n",
      "Time : 14.158915 seconds\n",
      "step 922: train loss 2.6871, val loss 2.7553\n",
      "Time : 14.125875 seconds\n",
      "step 923: train loss 2.6804, val loss 2.7537\n",
      "Time : 14.105603 seconds\n",
      "step 924: train loss 2.6817, val loss 2.7555\n",
      "Time : 14.370911 seconds\n",
      "step 925: train loss 2.6878, val loss 2.7497\n",
      "Time : 14.355789 seconds\n",
      "step 926: train loss 2.6741, val loss 2.7492\n",
      "Time : 14.138894 seconds\n",
      "step 927: train loss 2.6768, val loss 2.7526\n",
      "Time : 14.108013 seconds\n",
      "step 928: train loss 2.6783, val loss 2.7466\n",
      "Time : 14.156062 seconds\n",
      "step 929: train loss 2.6787, val loss 2.7498\n",
      "Time : 14.180740 seconds\n",
      "step 930: train loss 2.6829, val loss 2.7501\n",
      "Time : 14.136948 seconds\n",
      "step 931: train loss 2.6864, val loss 2.7525\n",
      "Time : 14.105690 seconds\n",
      "step 932: train loss 2.6777, val loss 2.7512\n",
      "Time : 14.217786 seconds\n",
      "step 933: train loss 2.6777, val loss 2.7517\n",
      "Time : 14.319242 seconds\n",
      "step 934: train loss 2.6811, val loss 2.7538\n",
      "Time : 14.251192 seconds\n",
      "step 935: train loss 2.6835, val loss 2.7519\n",
      "Time : 14.133426 seconds\n",
      "step 936: train loss 2.6796, val loss 2.7450\n",
      "Time : 14.123375 seconds\n",
      "step 937: train loss 2.6834, val loss 2.7457\n",
      "Time : 14.167793 seconds\n",
      "step 938: train loss 2.6840, val loss 2.7552\n",
      "Time : 14.199593 seconds\n",
      "step 939: train loss 2.6855, val loss 2.7588\n",
      "Time : 14.137483 seconds\n",
      "step 940: train loss 2.6846, val loss 2.7528\n",
      "Time : 14.158511 seconds\n",
      "step 941: train loss 2.6803, val loss 2.7582\n",
      "Time : 14.319183 seconds\n",
      "step 942: train loss 2.6727, val loss 2.7520\n",
      "Time : 14.304480 seconds\n",
      "step 943: train loss 2.6765, val loss 2.7536\n",
      "Time : 14.136888 seconds\n",
      "step 944: train loss 2.6759, val loss 2.7556\n",
      "Time : 14.115097 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 945: train loss 2.6750, val loss 2.7505\n",
      "Time : 14.159231 seconds\n",
      "step 946: train loss 2.6765, val loss 2.7469\n",
      "Time : 14.153757 seconds\n",
      "step 947: train loss 2.6794, val loss 2.7519\n",
      "Time : 14.113166 seconds\n",
      "step 948: train loss 2.6845, val loss 2.7464\n",
      "Time : 14.118094 seconds\n",
      "step 949: train loss 2.6792, val loss 2.7559\n",
      "Time : 14.270528 seconds\n",
      "step 950: train loss 2.6825, val loss 2.7484\n",
      "Time : 14.365271 seconds\n",
      "step 951: train loss 2.6823, val loss 2.7520\n",
      "Time : 14.247933 seconds\n",
      "step 952: train loss 2.6793, val loss 2.7483\n",
      "Time : 14.109315 seconds\n",
      "step 953: train loss 2.6779, val loss 2.7503\n",
      "Time : 14.183018 seconds\n",
      "step 954: train loss 2.6814, val loss 2.7480\n",
      "Time : 14.174367 seconds\n",
      "step 955: train loss 2.6789, val loss 2.7559\n",
      "Time : 14.157983 seconds\n",
      "step 956: train loss 2.6882, val loss 2.7495\n",
      "Time : 14.114030 seconds\n",
      "step 957: train loss 2.6818, val loss 2.7507\n",
      "Time : 14.137438 seconds\n",
      "step 958: train loss 2.6762, val loss 2.7463\n",
      "Time : 14.322346 seconds\n",
      "step 959: train loss 2.6741, val loss 2.7455\n",
      "Time : 14.300124 seconds\n",
      "step 960: train loss 2.6861, val loss 2.7464\n",
      "Time : 14.121609 seconds\n",
      "step 961: train loss 2.6786, val loss 2.7506\n",
      "Time : 14.117612 seconds\n",
      "step 962: train loss 2.6786, val loss 2.7497\n",
      "Time : 14.154105 seconds\n",
      "step 963: train loss 2.6790, val loss 2.7453\n",
      "Time : 14.144413 seconds\n",
      "step 964: train loss 2.6814, val loss 2.7494\n",
      "Time : 14.191958 seconds\n",
      "step 965: train loss 2.6856, val loss 2.7485\n",
      "Time : 14.147768 seconds\n",
      "step 966: train loss 2.6821, val loss 2.7487\n",
      "Time : 14.261614 seconds\n",
      "step 967: train loss 2.6732, val loss 2.7475\n",
      "Time : 14.277782 seconds\n",
      "step 968: train loss 2.6773, val loss 2.7510\n",
      "Time : 14.183749 seconds\n",
      "step 969: train loss 2.6821, val loss 2.7567\n",
      "Time : 14.135235 seconds\n",
      "step 970: train loss 2.6765, val loss 2.7472\n",
      "Time : 14.150728 seconds\n",
      "step 971: train loss 2.6773, val loss 2.7512\n",
      "Time : 14.175826 seconds\n",
      "step 972: train loss 2.6803, val loss 2.7585\n",
      "Time : 14.162838 seconds\n",
      "step 973: train loss 2.6852, val loss 2.7549\n",
      "Time : 14.100279 seconds\n",
      "step 974: train loss 2.6858, val loss 2.7465\n",
      "Time : 14.163322 seconds\n",
      "step 975: train loss 2.6760, val loss 2.7456\n",
      "Time : 14.387901 seconds\n",
      "step 976: train loss 2.6799, val loss 2.7375\n",
      "Time : 14.351182 seconds\n",
      "step 977: train loss 2.6827, val loss 2.7460\n",
      "Time : 14.111090 seconds\n",
      "step 978: train loss 2.6802, val loss 2.7457\n",
      "Time : 14.100397 seconds\n",
      "step 979: train loss 2.6747, val loss 2.7460\n",
      "Time : 14.143318 seconds\n",
      "step 980: train loss 2.6746, val loss 2.7520\n",
      "Time : 14.181282 seconds\n",
      "step 981: train loss 2.6812, val loss 2.7505\n",
      "Time : 14.115471 seconds\n",
      "step 982: train loss 2.6790, val loss 2.7504\n",
      "Time : 14.133884 seconds\n",
      "step 983: train loss 2.6799, val loss 2.7508\n",
      "Time : 14.251029 seconds\n",
      "step 984: train loss 2.6778, val loss 2.7458\n",
      "Time : 14.316750 seconds\n",
      "step 985: train loss 2.6782, val loss 2.7498\n",
      "Time : 14.196897 seconds\n",
      "step 986: train loss 2.6846, val loss 2.7574\n",
      "Time : 14.133770 seconds\n",
      "step 987: train loss 2.6719, val loss 2.7421\n",
      "Time : 14.124228 seconds\n",
      "step 988: train loss 2.6667, val loss 2.7500\n",
      "Time : 14.151955 seconds\n",
      "step 989: train loss 2.6759, val loss 2.7461\n",
      "Time : 14.139752 seconds\n",
      "step 990: train loss 2.6751, val loss 2.7440\n",
      "Time : 14.158442 seconds\n",
      "step 991: train loss 2.6784, val loss 2.7460\n",
      "Time : 14.183573 seconds\n",
      "step 992: train loss 2.6859, val loss 2.7448\n",
      "Time : 14.314975 seconds\n",
      "step 993: train loss 2.6806, val loss 2.7541\n",
      "Time : 14.294352 seconds\n",
      "step 994: train loss 2.6801, val loss 2.7491\n",
      "Time : 14.143540 seconds\n",
      "step 995: train loss 2.6747, val loss 2.7530\n",
      "Time : 14.149302 seconds\n",
      "step 996: train loss 2.6817, val loss 2.7491\n",
      "Time : 14.196284 seconds\n",
      "step 997: train loss 2.6855, val loss 2.7536\n",
      "Time : 14.178992 seconds\n",
      "step 998: train loss 2.6752, val loss 2.7478\n",
      "Time : 14.115002 seconds\n",
      "step 999: train loss 2.6791, val loss 2.7507\n",
      "Time : 14.119931 seconds\n"
     ]
    }
   ],
   "source": [
    "all_train_losses = []\n",
    "all_val_losses = []\n",
    "for iter in range(max_iters):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    #if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "    losses = estimate_loss()\n",
    "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    all_train_losses.append(losses['train'])\n",
    "    all_val_losses.append(losses['val'])\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Time : {:2f} seconds\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf9087",
   "metadata": {},
   "source": [
    "Create samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd397621-3e6f-4fd7-ac88-919e6560db70",
   "metadata": {
    "id": "bd397621-3e6f-4fd7-ac88-919e6560db70",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! dis Moule Picautris ! Cest encore la premire chose ! Je ne parle pas en ce moment que le Victor nest pas un homme que Favras ; qui devient glissent  terre. La tyrannie a t si brave, elle est partie, javais des objets  ma bonneauxse\n",
      "\n",
      "Schmisse coutez, qui va ne si, sursis de se faire effacer de M. le jette immoral  ltat. Je rappelle le procureur Vent-moi bien et jeune fille\n",
      "\n",
      "\n",
      "\n",
      "Grand bien extraordinaire, bien des reus et les montagnes tranquilles, mais le voil tout, chreomaques, jusqu sa famille des hommes, et aprs toute mon absence il mattendrait. Il y avait dans la valle de M. bal. Deux endurzes distroite taient perms pour moi, en feuillire les ch\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "\n",
    "print(tokenizers.decode(m.generate(context, max_new_tokens=250)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e3ff9527-53fa-4029-a42a-3c799961a738",
   "metadata": {
    "id": "e3ff9527-53fa-4029-a42a-3c799961a738",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sous un soleil, tait li, longueurs des vieillards. Je dormais debout. Tout en deux av rubans, et contaient un pavillon retentissait dans la rubitation, jy fis certaines gens qui me paraissaient ses lignoirs ardeurs  cette fentre ou du silence. Lucien Hector ne se dtachait sa part la premire remercitive de lui, fit jusqu lternit du cadavre de lhonneur. Je ntais pas trs cher, Pierre.\n",
      "\n",
      "Clrait, bien encore aprs le doux du sang qui sortait avec rsultat ses rayons infimales, entourait le ciel dans la rivire ; celui-ci,  et, dans le bois  la colline,  lordonafouchon, une troisime ou large voiture dans laquelle ils un fort  remplir des circonstances d\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor([tokenizers.encode(\"Sous un soleil, \")], dtype=torch.long, device=device)\n",
    "\n",
    "print(tokenizers.decode(m.generate(context, max_new_tokens=250)[0].tolist()))\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1ffbd",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c4883f1",
   "metadata": {
    "id": "042c197c-d4c7-46a6-8778-b54a58d57e2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save in: saved-tiktoken-64batch-128block-4000-ite-x_xx\n"
     ]
    }
   ],
   "source": [
    "last_iteration = extract_iteration_count_from(last_backup_file)\n",
    "current_iteration = last_iteration + max_iters\n",
    "current_save = \"{}{}-ite-x_xx\".format(backup_prefix, current_iteration)\n",
    "print(\"Save in:\",current_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c6512e8",
   "metadata": {
    "id": "042c197c-d4c7-46a6-8778-b54a58d57e2a"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), current_save)\n",
    "\n",
    "append_losses_to_file(\"saved-tiktoken-train-losses.txt\", all_train_losses)\n",
    "append_losses_to_file(\"saved-tiktoken-val-losses.txt\", all_val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe2eda",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d7ce2ed-f0cf-4eb4-bae1-698e664626b9",
   "metadata": {
    "id": "4d7ce2ed-f0cf-4eb4-bae1-698e664626b9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load array from file and convert to PyTorch tensor\n",
    "train_losses = np.loadtxt('saved-tiktoken-train-losses.txt')\n",
    "val_losses = np.loadtxt('saved-tiktoken-val-losses.txt')\n",
    "#my_tensor = torch.from_numpy(my_array)\n",
    "\n",
    "\n",
    "def plot_losses(losses1, losses2):\n",
    "    # create a new figure\n",
    "    plt.figure()\n",
    "\n",
    "    # plot the first series of losses\n",
    "    plt.plot(losses1, label='Train loss')\n",
    "\n",
    "    # plot the second series of losses\n",
    "    plt.plot(losses2, label='Val loss')\n",
    "\n",
    "    # add labels and title to the plot\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Losses')\n",
    "\n",
    "    # add a legend to the plot\n",
    "    plt.legend()\n",
    "\n",
    "    # display the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac6c92",
   "metadata": {},
   "source": [
    "## Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7cc1e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "del m\n",
    "del model\n",
    "del optimizer\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091044d8",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
